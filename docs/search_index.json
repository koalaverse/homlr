[
["index.html", "Hands-on Machine Learning with R Preface Who should read this Why R Structure of the book Conventions used in this book Additional resources Feedback Acknowledgments Software information", " Hands-on Machine Learning with R 2018-08-01 Preface Welcome to Hands-on Machine Learning with R. This book provides hands-on modules for many of the most common machine learning methods to include: Generalized low rank models Clustering algorithms Autoencoders Regularized models Random forests Gradient boosting machines Deep neural networks Stacking / super learner and more! You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, my motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. For the most part, I minimize mathematical complexity when possible but also provide resources to get deeper into the details if desired. Who should read this I intend this work to be a practitioner‚Äôs guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, I‚Äôve long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book. This book is not meant to be an introduction to R or to programming in general; as I assume the reader has familiarity with the R language to include defining functions, managing R objects, controlling the flow of a program, and other basic tasks. If not, I would refer you to R for Data Science (Wickham and Grolemund 2016) to learn the fundamentals of data science with R such as importing, cleaning, transforming, visualizing, and exploring your data. For those looking to advance their R programming skills and knowledge of the languge, I would refer you to Advanced R (Wickham 2014). Nor is this book designed to be a deep dive into the theory and math underpinning machine learning algorithms. Several books already exist that do great justice in this arena (i.e. Elements of Statistical Learning (Friedman, Hastie, and Tibshirani 2001), Computer Age Statistical Inference (Kuhn and Johnson 2013), Deep Learning (Goodfellow et al. 2016)). Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as glmnet, h20, ranger, xgboost, lime, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, I highly recommend you experiment with the code examples provided throughout. Why R R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing statistical methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: tidyverse for common data analysis activities; h2o, ranger, xgboost, and others for fast and scalable machine learning; lime, pdp, DALEX, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow. Structure of the book Each chapter of this book focuses on a particular part of the machine learning process along with various packages to perform that process. TBD‚Ä¶ Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following code chunks with images, which signify: Signifies a tip or suggestion Signifies a general note Signifies a warning or caution Additional resources There are many great resources available to learn about machine learning. At the end of each chapter I provide a Learn More section that lists resources that I have found extremely useful for digging deeper into the methodology and applying with code. Feedback Reader comments are greatly appreciated. To report errors or bugs please post an issue at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r/issues. Acknowledgments TBD Software information An online version of this book is available at https://bradleyboehmke.github.io/hands-on-machine-learning-with-r/. The source of the book is available at https://github.com/bradleyboehmke/hands-on-machine-learning-with-r. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with the following packages and R version. All code was executed on 2013 MacBook Pro with a 2.4 GHz Intel Core i5 processor, 8 GB of memory, 1600MHz speed, and double data rate synchronous dynamic random access memory (DDR3). # packages used pkgs &lt;- c( &quot;AmesHousing&quot;, &quot;caret&quot;, &quot;data.table&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;glmnet&quot;, &quot;h2o&quot;, &quot;pROC&quot;, &quot;purrr&quot;, &quot;ROCR&quot;, &quot;rsample&quot; ) # package &amp; session info devtools::session_info(pkgs) #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.5.1 (2018-07-02) #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/New_York #&gt; date 2018-08-01 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; abind 1.4-5 2016-07-21 CRAN (R 3.5.0) #&gt; AmesHousing 0.0.3 2017-12-17 CRAN (R 3.5.0) #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.5.0) #&gt; backports 1.1.2 2017-12-13 CRAN (R 3.5.0) #&gt; BH 1.66.0-1 2018-02-13 CRAN (R 3.5.0) #&gt; bindr 0.1.1 2018-03-13 CRAN (R 3.5.0) #&gt; bindrcpp 0.2.2 2018-03-29 CRAN (R 3.5.0) #&gt; bitops 1.0-6 2013-08-17 CRAN (R 3.5.0) #&gt; broom 0.5.0 2018-07-17 CRAN (R 3.5.0) #&gt; caret 6.0-80 2018-05-26 CRAN (R 3.5.0) #&gt; caTools 1.17.1 2014-09-10 CRAN (R 3.5.0) #&gt; class 7.3-14 2015-08-30 CRAN (R 3.5.1) #&gt; cli 1.0.0 2017-11-05 CRAN (R 3.5.0) #&gt; codetools 0.2-15 2016-10-05 CRAN (R 3.5.1) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.5.0) #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.5.0) #&gt; CVST 0.2-2 2018-05-26 CRAN (R 3.5.0) #&gt; data.table 1.11.4 2018-05-27 CRAN (R 3.5.0) #&gt; ddalpha 1.3.3 2018-04-30 CRAN (R 3.5.0) #&gt; DEoptimR 1.0-8 2016-11-19 CRAN (R 3.5.0) #&gt; dichromat 2.0-0 2013-01-24 CRAN (R 3.5.0) #&gt; digest 0.6.15 2018-01-28 CRAN (R 3.5.0) #&gt; dimRed 0.1.0 2017-05-04 CRAN (R 3.5.0) #&gt; dplyr 0.7.6 2018-06-29 cran (@0.7.6) #&gt; DRR 0.0.3 2018-01-06 CRAN (R 3.5.0) #&gt; fansi 0.2.3 2018-05-06 cran (@0.2.3) #&gt; foreach 1.4.4 2017-12-12 CRAN (R 3.5.0) #&gt; gdata 2.18.0 2017-06-06 CRAN (R 3.5.0) #&gt; geometry 0.3-6 2015-09-09 CRAN (R 3.5.0) #&gt; ggplot2 3.0.0 2018-07-03 CRAN (R 3.5.0) #&gt; glmnet 2.0-16 2018-04-02 CRAN (R 3.5.0) #&gt; glue 1.3.0 2018-07-23 Github (tidyverse/glue@66de125) #&gt; gower 0.1.2 2017-02-23 CRAN (R 3.5.0) #&gt; gplots 3.0.1 2016-03-30 CRAN (R 3.5.0) #&gt; graphics * 3.5.1 2018-07-05 local #&gt; grDevices * 3.5.1 2018-07-05 local #&gt; grid 3.5.1 2018-07-05 local #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.5.0) #&gt; gtools 3.5.0 2015-05-29 CRAN (R 3.5.0) #&gt; h2o 3.18.0.11 2018-05-24 CRAN (R 3.5.0) #&gt; ipred 0.9-6 2017-03-01 CRAN (R 3.5.0) #&gt; iterators 1.0.9 2017-12-12 CRAN (R 3.5.0) #&gt; jsonlite 1.5 2017-06-01 CRAN (R 3.5.0) #&gt; kernlab 0.9-26 2018-04-30 CRAN (R 3.5.0) #&gt; KernSmooth 2.23-15 2015-06-29 CRAN (R 3.5.1) #&gt; labeling 0.3 2014-08-23 CRAN (R 3.5.0) #&gt; lattice 0.20-35 2017-03-25 CRAN (R 3.5.1) #&gt; lava 1.6.1 2018-03-28 CRAN (R 3.5.0) #&gt; lazyeval 0.2.1 2017-10-29 CRAN (R 3.5.0) #&gt; lubridate 1.7.4 2018-04-11 CRAN (R 3.5.0) #&gt; magic 1.5-8 2018-01-26 CRAN (R 3.5.0) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.5.0) #&gt; MASS 7.3-50 2018-04-30 CRAN (R 3.5.1) #&gt; Matrix 1.2-14 2018-04-13 CRAN (R 3.5.1) #&gt; methods * 3.5.1 2018-07-05 local #&gt; mgcv 1.8-24 2018-06-23 CRAN (R 3.5.1) #&gt; ModelMetrics 1.1.0 2016-08-26 CRAN (R 3.5.0) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.5.0) #&gt; nlme 3.1-137 2018-04-07 CRAN (R 3.5.1) #&gt; nnet 7.3-12 2016-02-02 CRAN (R 3.5.1) #&gt; numDeriv 2016.8-1 2016-08-27 CRAN (R 3.5.0) #&gt; pillar 1.3.0 2018-07-14 cran (@1.3.0) #&gt; pkgconfig 2.0.1 2017-03-21 CRAN (R 3.5.0) #&gt; plogr 0.2.0 2018-03-25 CRAN (R 3.5.0) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.5.0) #&gt; pROC 1.12.1 2018-05-06 CRAN (R 3.5.0) #&gt; prodlim 2018.04.18 2018-04-18 CRAN (R 3.5.0) #&gt; purrr 0.2.5 2018-05-29 CRAN (R 3.5.0) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.5.0) #&gt; RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.5.0) #&gt; Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) #&gt; RcppRoll 0.2.2 2015-04-05 CRAN (R 3.5.0) #&gt; RCurl 1.95-4.10 2018-01-04 CRAN (R 3.5.0) #&gt; recipes 0.1.2 2018-01-11 CRAN (R 3.5.0) #&gt; reshape2 1.4.3 2017-12-11 CRAN (R 3.5.0) #&gt; rlang 0.2.1 2018-05-30 CRAN (R 3.5.0) #&gt; robustbase 0.93-0 2018-04-24 CRAN (R 3.5.0) #&gt; ROCR 1.0-7 2015-03-26 CRAN (R 3.5.0) #&gt; rpart 4.1-13 2018-02-23 CRAN (R 3.5.1) #&gt; rsample 0.0.2 2017-11-12 CRAN (R 3.5.0) #&gt; scales 0.5.0 2017-08-24 CRAN (R 3.5.0) #&gt; sfsmisc 1.1-2 2018-03-05 CRAN (R 3.5.0) #&gt; splines 3.5.1 2018-07-05 local #&gt; SQUAREM 2017.10-1 2017-10-07 CRAN (R 3.5.0) #&gt; stats * 3.5.1 2018-07-05 local #&gt; stats4 3.5.1 2018-07-05 local #&gt; stringi 1.2.4 2018-07-20 cran (@1.2.4) #&gt; stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) #&gt; survival 2.42-3 2018-04-16 CRAN (R 3.5.1) #&gt; tibble 1.4.2 2018-01-22 CRAN (R 3.5.0) #&gt; tidyr 0.8.1 2018-05-18 CRAN (R 3.5.0) #&gt; tidyselect 0.2.4 2018-02-26 CRAN (R 3.5.0) #&gt; timeDate 3043.102 2018-02-21 CRAN (R 3.5.0) #&gt; tools 3.5.1 2018-07-05 local #&gt; utf8 1.1.4 2018-05-24 CRAN (R 3.5.0) #&gt; utils * 3.5.1 2018-07-05 local #&gt; viridisLite 0.3.0 2018-02-01 CRAN (R 3.5.0) #&gt; withr 2.1.2 2018-03-15 CRAN (R 3.5.0) References "],
["intro.html", "Chapter 1 Introduction 1.1 Supervised Learning 1.2 Unsupervised Learning 1.3 Machine learning interpretability 1.4 The data sets", " Chapter 1 Introduction Machine learning continues to grow in importance for many organizations across nearly all domains. Examples include: predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge, segmenting customers based on common attributes or purchasing behavior for target marketing, predicting coupon redemption rates for a given marketing campaign, predicting customer churn so an organization can perform preventative intervention, and many more! In essence, these tasks all seek to learn from data. To address each scenario, we use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision provided during training. The two main groups this book focuses on includes: supervised learners that are used to construct predictive models, and unsupervised learners that are used to build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish. 1.1 Supervised Learning A predictive model is used for tasks that involve the prediction of a given output using other variables and their values (features) in the data set. Or as stated by Kuhn and Johnson (2013), predictive modeling is ‚Äúthe process of developing a mathematical tool or model that generates an accurate prediction‚Äù (p. 2). The learning algorithm in a predictive model attempts to discover and model the relationship among the target response (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks, using home attributes to predict the sales price, using employee attributes to predict the likelihood of attrition, using patient attributes and symptoms to predict the risk of readmission, using production attributes to predict time to market. Each of these examples have a defined learning task. They each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this text I will use various terms interchangeably for: \\(X\\): ‚Äúpredictor variables‚Äù, ‚Äúindependent variables‚Äù, ‚Äúattributes‚Äù, ‚Äúfeatures‚Äù, ‚Äúpredictors‚Äù \\(Y\\): ‚Äútarget variable‚Äù, ‚Äúdependent variable‚Äù, ‚Äúresponse‚Äù, ‚Äúoutcome measurement‚Äù The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the desired solutions. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Supervised learning problems revolve around two primary themes: regression and classification. 1.1.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuous numeric spectrum. In the examples above predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along the continuous spectrum. Figure 1.1 illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along the plane. Figure 1.1: Average home sales price as a function of year built and total square footage. 1.1.2 Classification problems When the objective of our supervised learning is to predict a categorical response, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: did a customer redeem a coupon (yes/no, 1/0), did a customer churn (yes/no, 1/0), did a customer click on our online ad (yes/no, 1/0), classifying customer reviews: binary: positive vs negative multinomial: extremely negative to extremely positive on a 0-5 Likert scale However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e. ‚Äúyes‚Äù or ‚Äúno‚Äù), we often predict the probability of a particular class (i.e. yes: .65, no: .35). Then the class with the highest probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. 1.1.3 Algorithm Comparison Guide TODO: keep this here or move reference guide to back??? Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, the supervised learning algorithms I cover in this book can be applied to both. These algorithms have become the most popular machine learning applications in recent years. Although the chapters that follow will go into detail on each algorithm, the following provides a quick reference guide that compares and contrasts some of their features. Moreover, I provide recommended base learner packages that I have found to scale well with typical rectangular data analyzed by organizations. Characteristics Regularized GLM Random Forest Gradient Boosting Machine Deep Learning Allows n &lt; p Provides automatic feature selection Handles missing values No feature pre-processing required Robust to outliers Easy to tune Computational speed Predictive power Preferred regression base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning Preferred classifciation base learner glmnet h2o.glm ranger h2o.randomForest xgboost h2o.gbm keras h2o.deeplearning 1.2 Unsupervised Learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables. For example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical regression models break down in the presence of highly correlated features. Dimension reduction techniques provide a method to reduce the feature set to a potentially smaller set of uncorrelated variables. These variables are often used as the input variables to downstream supervised models like. Unsupervised learning is often performed as part of an exploratory data analysis. However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e. linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don‚Äôt know the true answer‚Äîthe problem is unsupervised. However, the importance of unsupervised learning should not be overlooked and techniques for unsupervised learning are used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, often the results of an unsupervised model can be used as inputs to downstream supervised learning models. 1.2.1 Algorithm Decision Guide TBD 1.3 Machine learning interpretability In his seminal 2001 paper, Leo Breiman popularized the phrase: ‚Äúthe multiplicity of good models.‚Äù The phrase means that for the same set of input variables and prediction targets, complex machine learning algorithms can produce multiple accurate models with very similar, but not the exact same, internal architectures. Figure 1.2 is a depiction of a non-convex error surface that is representative of the error function for a machine learning algorithm with two inputs ‚Äî say, a customer‚Äôs income and a customer‚Äôs age, and an output, such as the same customer‚Äôs probability of redeeming a coupon. This non-convex error surface with no obvious global minimum implies there are many different ways complex machine learning algorithms could learn to weigh a customer‚Äôs income and age to make a good decision about if they are likely to redeem a coupon. Each of these different weightings would create a different function for making coupon redemption (and therefore marketing) decisions, and each of these different functions would have different explanations. Figure 1.2: Non-convex error surface with many local minimas. All of this is an obstacle to data scientists. On one hand, different models can have widely different predictions based on the same feature set. Even models built from the same algorithm but with different hyperparameters can lead to different results. Consequently, practitioners should understand how different implementations of algorithms differ, which can cause variance in their results (i.e. a default xgboost model can produce very different results from a default gbm model, even though they both implement gradient boosting machines). Alternatively, data scientists can experience very similar predictions from different models based on the same feature set. However, these models will have very different logic and structure leading to different interpretations. Consequently, practitioners should understand how to interpret different types of models. This book will provide you with a fundamental understanding to compare and contrast models and even package implementations of similiar algorithms. Several machine learning interpretability techniques will be demonstrated to help you understand what is driving model and prediction performance. This will allow you to be more effective and efficient in applying and understanding mutliple good models. 1.4 The data sets The XX data sets chosen for this book allow us to illustrate the different features of our machine learning algorithms. Since the goal of this book is to demonstrate how to implement R‚Äôs ML stack, I make the assumption that you have already spent significant time cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this book such as: feature selection: removing unnecessary variables and retaining only those variables you wish to include in your modeling process, recoding variable names and values so that they are meaningful and interpretable, recoding or removing missing values. Consequently, the exemplar data sets I use throughout this book have, for the most part, gone through the necessary cleaning process. These data sets are all freely available and include: Property sales information as described in De Cock (2011). problem type: supervised regression response variable: sale price (i.e. $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home access: provided by the AmesHousing package (Kuhn 2017) more details: See ?AmesHousing::ames_raw # access data ames &lt;- AmesHousing::make_ames() # initial dimension dim(ames) ## [1] 2930 81 # response variable head(ames$SalePrice) ## NULL You can see the entire data cleaning process to transform the raw Ames housing data (AmesHousing::ames_raw) to the final clean data (AmesHousing::make_ames) that we will use in machine learning algorithms throughout this book at: https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R Employee attrition information originally provided by IBM Watson Analytics Lab. problem type: supervised binomial classification response variable: Attrition (i.e. ‚ÄúYes‚Äù, ‚ÄúNo‚Äù) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) access: provided by the rsample package (Kuhn and Wickham 2017) more details: See ?rsample::attrition # access data attrition &lt;- rsample::attrition # initial dimension dim(attrition) ## [1] 1470 31 # response variable head(attrition$Attrition) ## [1] Yes No Yes No No No ## Levels: No Yes Image information for handwritten numbers originally presented to AT&amp;T Bell Lab‚Äôs to help build automatic mail-sorting machines for the USPS. Has been used since early 1990s to compare machine learning performance on pattern recognition (i.e. LeCun et al. (1990); LeCun et al. (1998); Cire≈üan, Meier, and Schmidhuber (2012)). Problem type: supervised multinomial classification response variable: V785 (i.e. numbers to predict: 0, 1, ‚Ä¶, 9) features: 784 observations: 60,000 (train) / 10,000 (test) objective: use attributes about the ‚Äúdarkness‚Äù of each of the 784 pixels in images of handwritten numbers to predict if the number is 0, 1, ‚Ä¶, or 9. access: see the code chunk that follows for download instructions more details: See online MNIST documentation # load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) # load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) # initial dimension dim(train) ## [1] 60000 785 # response variable head(train$V785) ## [1] 2 3 0 0 2 7 TODO: get unsupervised data sets for clustering and dimension reduction examples References "],
["regression-performance.html", "Chapter 2 Preparing for Supervised Machine Learning 2.1 Prerequisites 2.2 Data splitting 2.3 Feature engineering 2.4 Basic model formulation 2.5 Model tuning 2.6 Cross Validation for Generalization 2.7 Model evaluation", " Chapter 2 Preparing for Supervised Machine Learning Machine learning is a very iterative process. If performed and interpreted correctly, we can have great confidence in our outcomes. If not, the results will be useless. Approaching machine learning correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Before introducing specific algorithms, this chapter introduces concepts that are commonly required in the supervised machine learning process and that you‚Äôll see briskly covered in each chapter. 2.1 Prerequisites This chapter leverages the following packages. library(rsample) library(caret) library(h2o) library(dplyr) # turn off progress bars h2o.no_progress() # launch h2o h2o.init() ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmp0ENAPo/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmp0ENAPo/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 750 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_cki800 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) To illustrate some of the concepts, we will use the Ames Housing data and employee attrition data introduced in Chapter 1. Throughout this book, I‚Äôll demonstrate approaches with regular data frames. However, since many of the supervised machine learning chapters leverage the h2o package, we‚Äôll also show how to do some of the tasks with H2O objects. This requires your data to be in an H2O object, which you can convert any data frame easily with as.h2o. If you try to convert the original rsample::attrition data set to an H2O object an error will occur. This is because several variables are ordered factors and H2O has no way of handling this data type. Consequently, you must convert any ordered factors to unordered. # ames data ames &lt;- AmesHousing::make_ames() ames.h2o &lt;- as.h2o(ames) # attrition data churn &lt;- rsample::attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) churn.h2o &lt;- as.h2o(churn) 2.2 Data splitting 2.2.1 Spending our data wisely A major goal of the machine learning process is to find an algorithm \\(f(x)\\) that most accurately predicts future values (\\(y\\)) based on a set of inputs (\\(x\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we ‚Äúspend‚Äù our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we split our data into training and test data sets: Training Set: these data are used to train our algorithms and tune hyper-parameters. Test Set: having chosen a final model, these data are used to estimate its prediction error (generalization error). These data should not be used during model training! Figure 2.1: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-testing splits include 60% (training) - 40% (testing), 70%-30%, or 80%-20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep in mind that as your overall data set gets smaller, spending too much in training (\\(&gt;80\\%\\)) won‚Äôt allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting), sometimes too much spent in testing (\\(&gt;40\\%\\)) won‚Äôt allow us to get a good assessment of model parameters In today‚Äôs data-rich environment, typically, we are not lacking in the quantity of observations, so a 70-30 split is often sufficient. The two most common ways of splitting data include simple random sampling and stratified sampling. 2.2.2 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the percentage of data represented in your response variable (\\(y\\)). There are multiple ways to split our data. Here we show four options to produce a 70-30 split (note that setting the seed value allows you to reproduce your randomized splits): Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this book I will use the number 123 often for reproducibility but the number itself has no special meaning. # base R set.seed(123) index_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7)) train_1 &lt;- ames[index_1, ] test_1 &lt;- ames[-index_1, ] # caret package set.seed(123) index_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) train_2 &lt;- ames[index_2, ] test_2 &lt;- ames[-index_2, ] # rsample package set.seed(123) split_1 &lt;- initial_split(ames, prop = 0.7) train_3 &lt;- training(split_1) test_3 &lt;- testing(split_1) # h2o package split_2 &lt;- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123) train_4 &lt;- split_2[[1]] test_4 &lt;- split_2[[2]] Since this sampling approach will randomly sample across the distribution of \\(y\\) (Sale_Price in our example), you will typically result in a similar distribution between your training and test sets as illustrated below. Figure 2.2: Training (black) vs. test (red) distribution. 2.2.3 Stratified sampling However, if we want to explicitly control our sampling so that our training and test sets have similar \\(y\\) distributions, we can use stratified sampling. This is more common with classification problems where the reponse variable may be imbalanced (90% of observations with response ‚ÄúYes‚Äù and 10% with response ‚ÄúNo‚Äù). However, we can also apply to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will break \\(y\\) down into quantiles and randomly sample from each quantile. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets. The easiest way to perform stratified sampling on a response variable is to use the rsample package, where you specify the response variable to stratafy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling both our training and testing sets have approximately equal response distributions. # orginal response distribution table(churn$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # stratified sampling with the rsample package set.seed(123) split_strat &lt;- initial_split(churn, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) # consistent response ratio between train &amp; test table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.838835 0.161165 table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8386364 0.1613636 2.3 Feature engineering Feature engineering generally refers to the process of adding, deleting, and transforming the variables to be applied to your machine learning algorithms. Feature engineering is a significant process and requires you to spend substantial time understanding your data‚Ä¶or as Leo Breiman said ‚Äúlive with your data before you plunge into modeling.‚Äù Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm‚Äôs predictive ability. We will not cover all the potential ways of implementing feature engineering; however, we will cover a few fundamental pre-processing tasks that can significantly improve modeling performance. To learn more about feature engineering check out Feature Engineering for Machine Learning by Zheng and Casari (2018) and Max Kuhn‚Äôs upcoming book Feature Engineering and Selection: A Practical Approach for Predictive Models. 2.3.1 Response Transformation Although not a requirement, normalizing the distribution of the response variable by using a transformation can lead to a big improvement, especially for parametric models. As we saw in the data splitting section, our response variable Sale_Price is right skewed. ggplot(train_1, aes(x = Sale_Price)) + geom_density(trim = TRUE) + geom_density(data = test_1, trim = TRUE, col = &quot;red&quot;) Figure 2.3: Right skewed response variable. To normalize, we have a few options: Option 1: normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. # log transformation train_log_y &lt;- log(train_1$Sale_Price) test_log_y &lt;- log(test_1$Sale_Price) If your reponse has negative values then a log transformation will produce NaNs. If these negative values are small (between -0.99 and 0) then you can apply log1p, which adds 1 to the value prior to applying a log transformation. If your data consists of negative equal to or less than -1, use the Yeo Johnson transformation mentioned next. log(-.5) ## [1] NaN log1p(-.5) ## [1] -0.6931472 Option 2: use a Box Cox transformation. A Box Cox transformation is more flexible and will find the transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution. Be sure to compute the lambda on the training set and apply that same lambda to both the training and test set to minimize data leakage. # Box Cox transformation lambda &lt;- forecast::BoxCox.lambda(train_1$Sale_Price) train_bc_y &lt;- forecast::BoxCox(train_1$Sale_Price, lambda) test_bc_y &lt;- forecast::BoxCox(test_1$Sale_Price, lambda) We can see that in this example, the log transformation and Box Cox transformation both do about equally well in transforming our reponse variable to be normally distributed. Figure 2.4: Response variable transformations. Note that when you model with a transformed response variable, your predictions will also be in the transformed value. You will likely want to re-transform your predicted values back to their normal state so that decision-makers can interpret the results. The following code can do this for you: # log transform a value y &lt;- log(10) # re-transforming the log-transformed value exp(y) ## [1] 10 # Box Cox transform a value y &lt;- forecast::BoxCox(10, lambda) # Inverse Box Cox function inv_box_cox &lt;- function(x, lambda) { if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda) } # re-transforming the Box Cox-transformed value inv_box_cox(y, lambda) ## [1] 10 ## attr(,&quot;lambda&quot;) ## [1] -0.3067918 If your response has negative values, you can use the Yeo-Johnson transformation. To apply, use car::powerTransform to identify the lambda, car::yjPower to apply the transformation, and VGAM::yeo.johnson to apply the transformation and/or the inverse transformation. 2.3.2 Predictor Transformation 2.3.3 One-hot encoding Many models require all predictor variables to be numeric. Consequently, we need to transform any categorical variables into numeric representations so that these algorithms can compute. Some packages automate this process (i.e. h2o, glm, caret) while others do not (i.e. glmnet, keras). Furthermore, there are many ways to encode categorical variables as numeric representations (i.e. one-hot, ordinal, binary, sum, Helmert). The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding variable x in the following: id x 1 a 2 c 3 b 4 c 5 c 6 a 7 b 8 c results in the following representation: id x.a x.b x.c 1 1 0 0 2 0 0 1 3 0 1 0 4 0 0 1 5 0 0 1 6 1 0 0 7 0 1 0 8 0 0 1 This is called less than full rank encoding where we retain all variables for each level of x. However, this creates perfect collinearity which causes problems with some machine learning algorithms (i.e. generalized regression models, neural networks). Alternatively, we can create full-rank one-hot encoding by dropping one of the levels (level a has been dropped): id x.b x.c 1 0 0 2 0 1 3 1 0 4 0 1 5 0 1 6 0 0 7 1 0 8 0 1 If you needed to manually implement one-hot encoding yourself you can with caret::dummyVars. Sometimes you may have a feature level with very few observations and all these observations show up in the test set but not the training set. The benefit of using dummyVars on the full data set and then applying the result to both the train and test data sets is that it will guarantee that the same features are represented in both the train and test data. # full rank one-hot encode - recommended for generalized linear models and # neural networks full_rank &lt;- dummyVars( ~ ., data = ames, fullRank = TRUE) train_oh &lt;- predict(full_rank, train_1) test_oh &lt;- predict(full_rank, test_1) # less than full rank --&gt; dummy encoding dummy &lt;- dummyVars( ~ ., data = ames, fullRank = FALSE) train_oh &lt;- predict(dummy, train_1) test_oh &lt;- predict(dummy, test_1) Two things to note: since one-hot encoding adds new features it can significantly increase the dimensionality of our data. If you have a data set with many categorical variables and those categorical variables in turn have many unique levels, the number of features can explode. In these cases you may want to explore ordinal encoding of your data. if using h2o you do not need to explicity encode your categorical predictor variables but you can override the default encoding. This can be considered a tuning parameter as some encoding approaches will improve modeling accuracy over other encodings. See the encoding options for h2o here. 2.3.4 Standardizing Some models (K-NN, SVMs, PLS, neural networks) require that the predictor variables have the same units. Centering and scaling can be used for this purpose and is often referred to as standardizing the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Some packages have built-in arguments (i.e. h2o, caret) to standardize and some do not (i.e. glm, keras). If you need to manually standardize your variables you can use the preProcess function provided by the caret package. For example, here we center and scale our Ames predictor variables. It is important that you standardize the test data based on the training mean and variance values of each feature. This minimizes data leakage. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;center&quot;, &quot;scale&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.3.5 Alternative Feature Transformation There are some alternative transformations that you can perform: Normalizing the predictor variables with a Box Cox transformation can improve parametric model performance. Collapsing highly correlated variables with PCA can reduce the number of features and increase the stability of generalize linear models. However, this reduces the amount of information at your disposal and we show you how to use regularization as a better alternative to PCA. Removing near-zero or zero variance variables. Variables with vary little variance tend to not improve model performance and can be removed. preProcess provides many other transformation options which you can read more about here. For example, the following normalizes predictors with a Box Cox transformation, center and scales continuous variables, performs principal component analysis to reduce the predictor dimensions, and removes predictors with near zero variance. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;, &quot;nzv&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 2.4 Basic model formulation There are many packages to perform machine learning and there are almost always more than one to perform each algorithm (i.e. there are over 20 packages to perform random forests). There are pros and cons to each package; some may be more computationally efficient while others may have more hyperparameter tuning options. Future chapters will expose you to many of the packages and algorithms that perform and scale best to most organization‚Äôs problems and data sets. Just realize there are more ways than one to skin a üôÄ. For example, these three functions will all produce the same linear regression model output. lm.lm &lt;- lm(Sale_Price ~ ., data = train_1) lm.glm &lt;- glm(Sale_Price ~ ., data = train_1, family = gaussian) lm.caret &lt;- train(Sale_Price ~ ., data = train_1, method = &quot;lm&quot;) One thing you will notice throughout this guide is that we can specify our model formulation in different ways. In the above examples we use the model formulation (Sale_Price ~ . which says explain Sale_Price based on all features) approach. Alternative approaches, which you will see more often throughout this guide, are the matrix formulation and variable name specification approaches. Matrix formulation requires that we separate our response variable from our features. For example, in the regularization section we‚Äôll use glmnet which requires our features (x) and response (y) variable to be specified separately: # get feature names features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # create feature and response set train_x &lt;- train_1[, features] train_y &lt;- train_1$Sale_Price # example of matrix formulation glmnet.m1 &lt;- glmnet(x = train_x, y = train_y) Alternatively, h2o uses variable name specification where we provide all the data combined in one training_frame but we specify the features and response with character strings: # create variable names and h2o training frame y &lt;- &quot;Sale_Price&quot; x &lt;- setdiff(names(train_1), y) train.h2o &lt;- as.h2o(train_1) # example of variable name specification h2o.m1 &lt;- h2o.glm(x = x, y = y, training_frame = train.h2o) 2.5 Model tuning Hyperparameters control the level of model complexity. Some algorithms have many tuning parameters while others have only one or two. Tuning can be a good thing as it allows us to transform our model to better align with patterns within our data. For example, the simple illustration below shows how the more flexible model aligns more closely to the data than the fixed linear model. Figure 2.5: Tuning allows for more flexible patterns to be fit. However, highly tunable models can also be dangerous because they allow us to overfit our model to the training data, which will not generalize well to future unseen data. Figure 2.6: Highly tunable models can overfit if we are not careful. Throughout this guide we will demonstrate how to tune the different parameters for each model. One way to performing hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy. However, this would be very tedious work. An alternative approach is to perform a grid search. A grid search is an automated approach to searching across many combinations of hyperparameter values. Throughout this guide you will be exposed to different approaches to performing grid searches. 2.6 Cross Validation for Generalization Our goal is to not only find a model that performs well on training data but to find one that performs well on future unseen data. So although we can tune our model to reduce some error metric to near zero on our training data, this may not generalize well to future unseen data. Consequently, our goal is to find a model and its hyperparameters that will minimize error on held-out data. Let‚Äôs go back to this image‚Ä¶ Figure 2.7: Bias versus variance. The model on the left is considered rigid and consistent. If we provided it a new training sample with slightly different values, the model would not change much, if at all. Although it is consistent, the model does not accurately capture the underlying relationship. This is considered a model with high bias. The model on the right is far more inconsistent. Even with small changes to our training sample, this model would likely change significantly. This is considered a model with high variance. The model in the middle balances the two and, likely, will minimize the error on future unseen data compared to the high bias and high variance models. This is our goal. Figure 2.8: Bias-variance tradeoff. To find the model that balances the bias-variance tradeoff, we search for a model that minimizes a k-fold cross-validation error metric. k-fold cross-validation is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. The model is fit on \\(k-1\\) folds and then the held-out validation fold is used to compute the error. This procedure is repeated k times; each time, a different group of observations is treated as the validation set. This process results in k estimates of the test error (\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging these values, which provides us with an approximation of the error to expect on unseen data. Figure 2.9: Illustration of the k-fold cross validation process. The algorithms we cover in this guide all have built-in cross validation capabilities. One typically uses a 5 or 10 fold CV (\\(k = 5\\) or \\(k = 10\\)). For example, h2o implements CV with the nfolds argument: # example of 10 fold CV in h2o h2o.cv &lt;- h2o.glm( x = x, y = y, training_frame = train.h2o, nfolds = 10 ) 2.7 Model evaluation This leads us to our final topic, error metrics to evaluate performance. There are several metrics we can choose from to assess the error of a supervised machine learning model. The most common include: 2.7.1 Regression models MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\)). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Deviance: Short for mean residual deviance. In essence, it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). If the response variable distribution is gaussian, then it is equal to MSE. When not, it usually gives a more useful estimate of error. Objective: minimize MAE: Mean absolute error. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\\(MAE = \\frac{1}{n} \\sum^n_{i=1}(\\vert y_i - \\hat y_i \\vert)\\)). Objective: minimize RMSLE: Root mean squared logarithmic error. Similiar to RMSE but it performs a log() on the actual and predicted values prior to computing the difference (\\(RMSLE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(log(y_i + 1) - log(\\hat y_i + 1))^2}\\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. Objective: minimize \\(R^2\\): This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \\(R^2\\) than the other. You should not place too much emphasis on this metric. Objective: maximize Most models we assess in this guide will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its good to realize that certain situations warrant emphasis on some more than others. 2.7.2 Classification models Misclassification: This is the overall error. For example, say you are predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class high, 6 of class medium, and 4 of class low, then you misclassified 13 out of 90 observations resulting in a 14% misclassification rate. Objective: minimize Mean per class error: This is the average error rate for each class. For the above example, this would be the mean of \\(\\frac{3}{25}, \\frac{6}{30}, \\frac{4}{35}\\), which is 12%. If your classes are balanced this will be identical to misclassification. Objective: minimize MSE: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probabilty of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \\(MSE = 0.09^2 = 0.0081\\), if it is B \\(MSE = 0.93^2 = 0.8649\\), if it is C \\(MSE = 0.98^2 = 0.9604\\). The squared component results in large differences in probabilities for the true class having larger penalties. Objective: minimize Cross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. Objective: minimize Gini index: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. Objective: minimize When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does). Figure 2.10: Confusion matrix. We can extract different levels of performance from these measures. For example, given the classification matrix below we can assess the following: Accuracy: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \\(\\frac{TP + TN}{total} = \\frac{100+50}{165} = 0.91\\). Objective: maximize Precision: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \\(\\frac{TP}{TP + FP} = \\frac{100}{100+10} = 0.91\\). Objective: maximize Sensitivity (aka recall): How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \\(\\frac{TP}{TP + FN} = \\frac{100}{100+5} = 0.95\\). Objective: maximize Specificity: How accurately does the classifier classify actual non-events? Example: \\(\\frac{TN}{TN + FP} = \\frac{50}{50+10} = 0.83\\). Objective: maximize Figure 2.11: Example confusion matrix. AUC: Area under the curve. A good classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. Objective: maximize Figure 2.12: ROC curve. References "],
["regularized-regression.html", "Chapter 3 Regularized Regression 3.1 Prerequisites 3.2 Advantages &amp; Disadvantages 3.3 The Idea 3.4 Implementation: Regression 3.5 Implementation: Binary Classification 3.6 Implementation: Multinomial Classification 3.7 Learning More", " Chapter 3 Regularized Regression Generalized linear models (GLMs) such as ordinary least squares regression and logistic regression are simple and fundamental approaches for supervised learning. Moreover, when the assumptions required by GLMs are met, the coefficients produced are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in today‚Äôs world, data sets being analyzed typically have a large amount of features. As the number of features grow, our GLM assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our coefficients, which can reduce the variance and decrease out of sample error. 3.1 Prerequisites This chapter assumes you are familiary with basic idea behind linear and logistic regression. If not, two tutorials to get you up to speed include this linear regression tutorial and this logistic regression tutorial. This chapter leverages the following packages. Some of these packages are playing a supporting role while the main emphasis will be on the glmnet (Friedman, Hastie, and Tibshirani 2010) and h2o (Kraljevic 2018) packages. library(glmnet) # implementing regularized regression approaches library(h2o) # implementing regularized regression approaches library(rsample) # training vs testing data split library(dplyr) # basic data manipulation procedures library(ggplot2) # plotting 3.2 Advantages &amp; Disadvantages Advantages: Normal GLM models require that you have more observations than variables (\\(n&gt;p\\)); regularized regression allows you to model wide data where \\(n&lt;p\\). Minimizes the impact of multicollinearity. Provides automatic feature selection (at least when you apply a Lasso or elastic net penalty). Minimal hyperparameters making it easy to tune. Computationally efficient - relatively fast compared to other algorithms in this guide and does not require large memory. Disdvantages: Requires data pre-processing - requires all variables to be numeric (i.e. one-hot encode). However, h2o helps to automate this process. Does not handle missing data - must impute or remove observations with missing values. Not robust to outliers as they can still bias the coefficients. Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion). Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks). 3.3 The Idea The easiest way to understand regularized regression is to explain how it is applied to ordinary least squares regression (OLS). The objective of OLS regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. Illustrated below, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane). Figure 2.1: Fitted regression line using Ordinary Least Squares. More formally, this objective function is written as: \\[\\text{minimize} \\bigg \\{ SSE = \\sum^n_{i=1} (y_i - \\hat{y}_i)^2 \\bigg \\} \\tag{1}\\] The OLS objective function performs quite well when our data align to the key assumptions of OLS regression: Linear relationship Multivariate normality No autocorrelation Homoscedastic (constant variance in residuals) There are more observations (n) than features (p) (\\(n &gt; p\\)) No or little multicollinearity However, for many real-life data sets we have very wide data, meaning we have a large number of features (p) that we believe are informative in predicting some outcome. As p increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. Specifically, as p increases there are three main issues we most commonly run into: 3.3.1 1. Multicollinearity As p increases we are more likely to capture multiple features that have some multicollinearity. When multicollinearity exists, we often see high variability in our coefficient terms. For example, in our Ames data, Gr_Liv_Area and TotRms_AbvGrd are two variables that have a correlation of 0.808 and both variables are strongly correlated to our response variable (Sale_Price). When we fit a model with both these variables we get a positive coefficient for Gr_Liv_Area but a negative coefficient for TotRms_AbvGrd, suggesting one has a positive impact to Sale_Price and the other a negative impact. # fit with two strongly correlated variables lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area TotRms_AbvGrd ## 42767.6 139.4 -11025.9 However, if we refit the model with each variable independently, they both show a positive impact. However, the Gr_Liv_Area effect is now smaller and the TotRms_AbvGrd is positive with a much larger magnitude. # fit with just Gr_Liv_Area lm(Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Coefficients: ## (Intercept) Gr_Liv_Area ## 13289.6 111.7 # fit with just TotRms_Area lm(Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Call: ## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames) ## ## Coefficients: ## (Intercept) TotRms_AbvGrd ## 18665 25164 This is a common result when collinearity exists. Coefficients for correlated features become over-inflated and can fluctuate significantly. One consequence of these large fluctuations in the coefficient terms is overfitting, which means we have high variance in the bias-variance tradeoff space. Although an analyst can use tools such as variance inflaction factors (Myers 1990) to identify and remove those strongly correlated variables, it is not always clear which variable(s) to remove. Nor do we always wish to remove variables as this may be removing signal in our data. 3.3.2 2. Insufficient solution When the number of features exceed the number of observations (\\(p &gt; n\\)), the OLS solution matrix is not invertible. This causes significant issues because it means: (1) The least-squares estimates are not unique. In fact, there are an infinite set of solutions available and most of these solutions overfit the data. (2) In many instances the result will be computationally infeasible. Consequently, to resolve this issue an analyst can remove variables until \\(p &lt; n\\) and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn &amp; Johnson, 2013, pp. 43-47), it can be cumbersome and prone to errors. 3.3.3 3. Interpretability With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called hard threshholding feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a soft threshholding approach that slowly pushes a feature‚Äôs effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals. 3.3.4 Regularized Models When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as penalized models or shrinkage methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model. The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P). \\[\\text{minimize} \\big \\{ SSE + P \\big \\} \\tag{2}\\] This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). This concept generalizes to all GLM models. So far, we have be discussing OLS and the sum of squared errors. However, different models within the GLM family (i.e. logistic regression, Poisson regression) have different loss functions. Yet we can think of the penalty parameter all the same - it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model‚Äôs loss function. There are three types of penalty parameters we can implement: Ridge Lasso Elastic net, which is a combination of Ridge and Lasso 3.3.4.1 Ridge penalty Ridge regression (Hoerl and Kennard 1970) controls the coefficients by adding \\(\\lambda \\sum^p_{j=1} \\beta_j^2\\) to the objective function. This penalty parameter is also referred to as ‚Äú\\(L_2\\)‚Äù as it signifies a second-order penalty being used on the coefficients.1 \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\bigg \\} \\tag{3}\\] This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter \\(\\lambda\\). When \\(\\lambda = 0\\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \\(\\lambda \\rightarrow \\infty\\), the penalty becomes large and forces our coefficients to near zero. This is illustrated in Figure 3.1 where exemplar coefficients have been regularized with \\(\\lambda\\) ranging from 0 to over 8,000 (\\(log(8103) = 9\\)). Figure 3.1: Ridge regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Although these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when \\(\\lambda \\rightarrow 0\\). Furthermore, you‚Äôll notice the large negative parameter that fluctuates until \\(log(\\lambda) \\approx 2\\) where it then continuously skrinks to zero. This is indicitive of multicollinearity and likely illustrates that constraining our coefficients with \\(log(\\lambda) &gt; 2\\) may reduce the variance, and therefore the error, in our model. In essence, the ridge regression model pushes many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features get pushed to near zero. This allows us to reduce the noise in our data, which provides us more clarity in identifying the true signals in our model. However, a ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso or elastic net penalty may be preferable. 3.3.4.2 Lasso penalty The least absolute shrinkage and selection operator (lasso) model (Tibshirani 1996) is an alternative to the ridge penalty that has a small modification to the penalty in the objective function. Rather than the \\(L_2\\) penalty we use the following \\(L_1\\) penalty \\(\\lambda \\sum^p_{j=1} | \\beta_j|\\) in the objective function. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{4}\\] Whereas the ridge penalty approach pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients to zero as illustrated in Figure 3.2. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection. Figure 3.2: Lasso regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Numbers on top axis illustrate how many non-zero coefficients remain. In the figure above we see that when \\(log(\\lambda) = -5\\) all 15 variables are in the model, when \\(log(\\lambda) = -1\\) 12 variables are retained, and when \\(log(\\lambda) = 1\\) only 3 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal. 3.3.4.3 Elastic nets A generalization of the ridge and lasso penalties is the elastic net penalty (Zou and Hastie 2005), which combines the two penalties. \\[\\text{minimize } \\bigg \\{ SSE + \\lambda_1 \\sum^p_{j=1} \\beta_j^2 + \\lambda_2 \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\tag{5}\\] Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. 3.3.5 Tuning Regularized models are simple to tune as there are only two tuning parameters: Size of penalty (\\(\\lambda\\)): Controls how much we want to constrain our coefficients. Small penalties where \\(\\lambda\\) is close to zero allow our coefficients to be larger; however, larger values of \\(\\lambda\\) penalize our coefficients and forces them to take on smaller values. Hence, this parameter is often called the shrinkage parameter. Alpha: The alpha parameter tells our model to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)). 3.3.6 Package implementation There are a few packages that implement variants of regularized regression. You can find a comprehensive list on the CRAN Machine Learning Task View. However, the most popular implementations which we will cover in this chapter include: glmnet: The original implementation of regularized regression in R. The glmnet R package provides an extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial regression. A nice vignette is available here. Features include2: The code can handle sparse input-matrix formats, as well as range constraints on coefficients. Automatically standardizes your feature set. Built-in cross validation. The core of glmnet is a set of fortran subroutines, which make for very fast execution. The algorithms use coordinate descent with warm starts and active set iterations. Supports the following distributions: ‚Äúgaussian‚Äù,‚Äúbinomial‚Äù,‚Äúpoisson‚Äù,‚Äúmultinomial‚Äù,‚Äúcox‚Äù,‚Äúmgaussian‚Äù h2o: The h2o R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive online resource that includes methodology and code documentation along with tutorials. Features include: Fits both regularized and non-regularized GLMs. Automated feature pre-processing (one-hot encode &amp; standardization). Built-in cross validation. Built-in grid search capabilities. Supports the following distributions: ‚Äúguassian‚Äù, ‚Äúbinomial‚Äù, ‚Äúmultinomial‚Äù, ‚Äúordinal‚Äù, ‚Äúpoisson‚Äù, ‚Äúgamma‚Äù, ‚Äútweedie‚Äù. Distributed and parallelized computation on either a single node or a multi-node cluster. Automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance. 3.4 Implementation: Regression To illustrate various regularization concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 3.4.1 glmnet The glmnet package is a fast implementation, but it requires some extra processing up-front to your data if it‚Äôs not already represented as a numeric matrix. glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set. Furthermore, we use the model.matrix function on our feature set (see Matrix::sparse.model.matrix for increased efficiency on large dimension data). We also Box Cox transform our response variable due to its skeweness. The Box Cox transformation of the response variable is not required; however, parametric models such as regularized regression are sensitive to skewed values so it is always recommended to normalize your response variable. # Create training and testing feature matrices # we use model.matrix(...)[, -1] to discard the intercept train_x &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1] test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1] # Create training and testing response vectors # transform y based on skewness of training data train_y &lt;- log(ames_train$Sale_Price) test_y &lt;- log(ames_test$Sale_Price) 3.4.1.1 Basic implementation To apply a regularized model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. Behind the scenes, glmnet is doing two things that you should be aware of: It is essential that predictor variables are standardized when performing regularized regression. glmnet performs this for you. If you standardize your predictors prior to glmnet you can turn this argument off with standardize = FALSE. glmnet will perform ridge models across a wide range of \\(\\lambda\\) parameters, which are illustrated in the figure below. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.3: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). In fact, we can see the exact \\(\\lambda\\) values applied with ridge$lambda. Although you can specify your own \\(\\lambda\\) values, by default glmnet applies 100 \\(\\lambda\\) values that are data derived. glmnet has built-in functions to auto-generate the appropriate \\(\\lambda\\) values based on the data so the vast majority of the time you will have little need to adjust the default \\(\\lambda\\) values. We can also directly access the coefficients for a model using coef. glmnet stores all the coefficients for each model in order of largest to smallest \\(\\lambda\\). Due to the number of features, here I just peak at the two largest coefficients (Latitude &amp; Overall_QualVery_Excellent) features for the largest \\(\\lambda\\) (279.1035) and smallest \\(\\lambda\\) (0.02791035). You can see how the largest \\(\\lambda\\) value has pushed these coefficients to nearly 0. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 279.1035 254.3087 231.7166 211.1316 192.3752 175.2851 # small lambda results in large coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 100] ## Latitude Overall_QualVery_Excellent ## 0.60585376 0.09800466 # large lambda results in small coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 1] ## Latitude Overall_QualVery_Excellent ## 6.228028e-36 9.372514e-37 However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.4.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation (CV). cv.glmnet provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV. Here we perform a CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses MSE as the loss function but you can also use mean absolute error by changing the type.measure argument. # Apply CV Ridge regression to attrition data ridge &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 0 ) # Apply CV Ridge regression to attrition data lasso &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.4: 10-fold cross validation MSE for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. Our plots above illustrate the 10-fold CV mean squared error (MSE) across the \\(\\lambda\\) values. In both models we see a slight improvement in the MSE as our penalty \\(log(\\lambda)\\) gets larger , suggesting that a regular OLS model likely overfits our data. But as we constrain it further (continue to increase the penalty), our MSE starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum MSE and the largest \\(\\lambda\\) value within one standard error of the minimum MSE. # Ridge model min(ridge$cvm) # minimum MSE ## [1] 0.02147691 ridge$lambda.min # lambda for this min MSE ## [1] 0.1236602 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min MSE ## [1] 0.02488411 ridge$lambda.1se # lambda for this MSE ## [1] 0.6599372 # Lasso model min(lasso$cvm) # minimum MSE ## [1] 0.02411134 lasso$lambda.min # lambda for this min MSE ## [1] 0.003865266 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min MSE ## [1] 0.02819356 lasso$lambda.1se # lambda for this MSE ## [1] 0.01560415 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest MSE and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar MSEs; however, these plots illustrate that ridge is still using all 299 variables whereas the lasso model can get a similar MSE by reducing our feature set from 299 down to 131. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 63 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.5: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we‚Äôve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.6: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. To set up our tuning, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we‚Äôll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, I recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace = TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), mse_min = NA, mse_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective \\(\\lambda\\) values. This grid search took 41 seconds to compute. # perform grid search for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet(train_x, train_y, alpha = tuning_grid$alpha[i], foldid = fold_id) # extract MSE and lambda values tuning_grid$mse_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$mse_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha mse_min mse_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0230 0.0267 0.179 0.795 ## 2 0.1 0.0237 0.0285 0.0387 0.156 ## 3 0.2 0.0241 0.0289 0.0193 0.0856 ## 4 0.3 0.0243 0.0295 0.0129 0.0627 ## 5 0.4 0.0245 0.0295 0.00966 0.0470 ## 6 0.5 0.0246 0.0295 0.00773 0.0376 ## 7 0.6 0.0247 0.0301 0.00644 0.0344 ## 8 0.7 0.0247 0.0302 0.00552 0.0295 ## 9 0.8 0.0247 0.0302 0.00483 0.0258 ## 10 0.9 0.0247 0.0302 0.00429 0.0229 ## 11 1 0.0248 0.0304 0.00387 0.0206 If we plot the MSE ¬± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.003521887\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = mse_1se - mse_min) %&gt;% ggplot(aes(alpha, mse_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) Figure 3.7: MSE ¬± one standard error for different alpha penalty parameters. 3.4.1.3 Visual interpretation Regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. Consequently, this makes understanding variable relationships simple with regularized models. And since the predictors have all been standardized, comparing the coefficients against one another allows us to identify the most influential predictors. Those predictors with the largest positive coefficients have the largest positive impact on our response variable and those variables with really small negative coefficients have a very small negative impact on our response. Furthermore, there is no difference between the global and local model interpretation (see model interpretation chapter for details). We‚Äôll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots the top 100 most influential variables. You can see that many of the variables have coefficients closer to zero but there a handful of predictor variables that have noticable influence. For example, properties with above average latitude (northern end of Ames), are in the Green Hills neighborhood (which is actually in the south part of Ames), or have 2 extra miscellaneous garage features will have a positive influence on the sales price. Alternatively, properties that are zoned agricultural, have a home functional code of ‚ÄúSal‚Äù (salvage only) or ‚ÄúSev‚Äù (severely damaged), or have a pool quality assessment of ‚ÄúGood‚Äù tend have a negative influence on the sales price. Keep in mind these coefficients are for the standardized variable values. Consequently, the interpretation of these coefficient values are not as clear. Basically, for every unit the Latitude variable is above the mean value, the reponse variable (which has been log transformed) has a 0.474 unit increase. The important insight is to identify those variables with the largest positive and negative impacts on the response variable. coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% top_n(100, wt = abs(value)) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Top 100 influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.8: A ridge penalty will retain all variables but push most coefficients to near zero. Consequently, we retain any minor signals that all features provide but we can visualize those coefficients with the largest absolute values to identify the most influential predictors. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum MSE and plot all variables in that model we see similar results to the ridge regarding the most influential variables. Although the ordering typically differs, we often see some commonality between lasso and ridge models regarding the most influential variables. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.9: A lasso penalty will perform feature selection by pushing most coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.4.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. The only caveat is you need to supply predict an s parameter with the preferred models \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum MSE of 0.02398 (RMSE = 0.123). However, our response variable is log transformed so we must re-transform it to get an interpretable RMSE (our average generalized error is $25,156.77). # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.02529035 # predict and get RMSE pred &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) caret::RMSE(pred, test_y) ## [1] 0.1220084 # re-transform predicted values and get interpretable RMSE caret::RMSE(exp(pred), exp(test_y)) ## [1] 24740.36 3.4.2 h2o To perform regularized regression with h2o, we first need to initiate our h2o session. ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpDYDdAT/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpDYDdAT/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 373 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do want to normalize our response variable due to its skewness and then convert our training and test data to h2o objects. # convert training data to h2o object train_h2o &lt;- ames_train %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- ames_test %&gt;% mutate(Sale_Price = log(Sale_Price)) %&gt;% as.h2o() # set the response column to Sale_Price response &lt;- &quot;Sale_Price&quot; # set the predictor names predictors &lt;- setdiff(colnames(ames_train), response) 3.4.2.1 Basic implementation h2o.glm allows us to perform a generalized linear model. If the response variable is continuous, h2o.glm will use a gaussian distribution (see family parameter ?h2o.glm). By default, h2o.glm performs an elastic net model with alpha = .5. Similar to glmnet, h2o.glm will perform an automated search across internally generated lambda values. You can override the automated lambda search by supplying different values to the lambda parameters in h2o.glm but this is not recommended as the default parameters typically perform best. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;gaussian&quot; ) # print the MSE and RMSE for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.0405933 h2o.rmse(h2o_fit1, xval = TRUE) ## [1] 0.2014778 If we check out the summary results of our model we get a bunch of information. Below is truncated printout which provides important model information such as the alpha applied and the optimal lambda value identified (\\(\\lambda = 0.056\\)), the number of predictors retained with these penalty parameters (11), and performance results for the training and validation sets. summary(h2o_fit1) ## Model Details: ## ============== ## ## H2ORegressionModel: glm ## Model Key: GLM_model_R_1531935157122_1 ## GLM Model: summary ## family link regularization number_of_predictors_total number_of_active_predictors number_of_iterations #training_frame ## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05581 ) 345 11 2 file13d1f4ffcacf3_sid_be66_16 ## ## H2ORegressionMetrics: glm ## ** Reported on training data. ** ## ## MSE: 0.03800294 ## RMSE: 0.1949434 ## MAE: 0.1277845 ## RMSLE: 0.01521899 ## Mean Residual Deviance : 0.03800294 ## R^2 : 0.7737851 ## Null Deviance :345.0615 ## Null D.o.F. :2053 ## Residual Deviance :78.05805 ## Residual D.o.F. :2042 ## AIC :-861.7687 ## ## ## ## H2ORegressionMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.0405933 ## RMSE: 0.2014778 ## MAE: 0.1310502 ## RMSLE: 0.01567771 ## Mean Residual Deviance : 0.0405933 ## R^2 : 0.7583658 ## Null Deviance :345.4348 ## Null D.o.F. :2053 ## Residual Deviance :83.37864 ## Residual D.o.F. :2042 ## AIC :-726.3293 ## ## truncated..... ## 3.4.2.2 Tuning As previously stated, a full grid search to identify the optimal alpha is not always necessary; changing its value to 0.5 (or 0 or 1 if we only want Ridge or Lasso, respectively) works in most cases. However, if a full grid search is desired then we need to supply our grid of alpha values in a list. We can then use h2o.grid to perform our grid search. The results show that \\(\\alpha = 0\\) (a full ridge penalty) performed best. This grid search took 5 seconds to compute. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, grid_id = &quot;grid_search&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## alpha model_ids mse ## 1 [0.0] grid_search_model_0 0.022711429277773212 ## 2 [0.6] grid_search_model_6 0.0395992947950626 ## 3 [0.7] grid_search_model_7 0.03968294882574923 ## 4 [1.0] grid_search_model_10 0.03983076501706063 ## 5 [0.4] grid_search_model_4 0.039850863900647 ## 6 [0.8] grid_search_model_8 0.03990607242146858 ## 7 [0.3] grid_search_model_3 0.04019054535587161 ## 8 [0.5] grid_search_model_5 0.04042328742741519 ## 9 [0.9] grid_search_model_9 0.04044324344598643 ## 10 [0.2] grid_search_model_2 0.041434990130949347 ## 11 [0.1] grid_search_model_1 0.04233746428242222 We can check out more details of the best performing model. Our RMSE (0.1279) is an improvement on our default model (RMSE = 0.2015). Also, we can access the optimal model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0279. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model) ## [1] 0.0163625 h2o.rmse(best_model) ## [1] 0.127916 # get optimal parameters best_model@parameters$lambda ## [1] 0.02790355 best_model@parameters$alpha ## [1] 0 3.4.2.3 Visual interpretation h2o provides a built-in function that plots variable importance. To compute variable importance for regularized models, h2o uses the standardized coefficient values (which is the same that we plotted in the glmnet example). You will notice that the the largest influential variables produced by this H2O model differ from the glmnet model. This is because we are assessing the ridge model here, where in the glmnet interpretation section we assessed the lasso and H2O and glmnet use differing approaches to generate the \\(\\lambda\\) search path. However, you will notice that both Overall_Qual.Excellent and Overall_Cond.Fair were also top influencers in the glmnet model suggesting they may have a strong signal regardless of the regularization penalty we use. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.10: H2O‚Äôs variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). Although H2O‚Äôs variable importance uses standardized coefficients, a convenient function of H2O is that you can extract the ‚Äúnormal‚Äù coefficients which are obtained from the standardized coefficients by reversing the data standardization process (de-scaled, with the intercept adjusted by an added offset) so that they can be applied to data in its original form (i.e. no standardization prior to scoring). For example, every one unit increase in pool size has a $1 decrease in sales price (since our response is log transformed we need to take the exponent \\(e^{-0.000183} = 0.999817\\)). These are not the same as coefficients of a model built on non-standardized data. best_model@model$coefficients_table ## Coefficients: glm coefficients ## names coefficients standardized_coefficients ## 1 Intercept -47.473537 11.713031 ## 2 Neighborhood.Bloomington_Heights -0.019249 -0.019249 ## 3 Neighborhood.Blueste 0.004638 0.004638 ## 4 Neighborhood.Briardale -0.009169 -0.009169 ## 5 Neighborhood.Brookside 0.008538 0.008538 ## ## --- ## names coefficients standardized_coefficients ## 341 Pool_Area -0.000183 -0.005963 ## 342 Misc_Val -0.000038 -0.022608 ## 343 Mo_Sold 0.000444 0.001224 ## 344 Year_Sold -0.007414 -0.009728 ## 345 Longitude -0.303402 -0.007786 ## 346 Latitude 0.948544 0.017592 In the glmnet section we discussed how regularized regression assumes a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. This constant change is given by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. We can illustrate this with a partial dependence plot of the ground living area (square footage) variable. The PDP plot shows that the relationship is monotonic linear (assumes a constant increasing relationships). This PDP plot helps to show the typical values (and one standard error) of our response variable as the square footage of the ground floor living space increases. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Gr_Liv_Area&quot;) Figure 3.11: As the ground living area (square footage) of a home increases, we experience a constant increase in the mean predicted sale price. But what about the two most influential variables (Overall_Qual.Excellent and Overall_Cond.Fair)? These variables are a result of one-hot encoding the original overall quality (Overall_Qual) variable. We can assess a similar plot but must supply the original non-one-hot encoded variable name. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;) Figure 3.12: The mean predicted sale price for each level of the overall quality variable. Note how the plot does not align with the natural ordering of the predictor variable. Unfortunately, H2O‚Äôs function plots the categorical levels in alphabetical order. Alternatively, we can extract the results and plot them in their proper level order to make inference more intuitive. The following shows the marginal effect of the overall quality variable on sales price. It illustrates an interesting finding - the highest and lowest categories do not have the largest marginal sales price effects. It also shows that a house with a very good overall quality has, on average, a $6K higher sale price than a house with only a good overall quality. Alternatively, homes with below average quality receive a substantially lower sale price than homes with average quality. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Overall_Qual&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate( Overall_Qual = factor(Overall_Qual, levels = levels(ames$Overall_Qual)), mean_response = exp(mean_response)) %&gt;% ggplot(aes(mean_response, Overall_Qual)) + geom_point() + scale_x_continuous(labels = scales::dollar) + ggtitle(&quot;Average response for each Overall Quality level&quot;) Figure 3.13: The mean predicted sale price for each level of the overall quality variable. This plot now helps to illustrate how the mean predicted sale price changes based on the natural ordering of the overall quality predictor variable. See the model interpretation chapter for more details on variable importance and partial dependence plots. 3.4.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Similar to glmnet, we need to re-transform our predicted values to get a interpretable generalizable error. Our generalizable error is $24,147.60 which is about $1,000 lower than the glmnet model produced. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict ## 1 11.65765 ## 2 11.48417 ## 3 12.50938 ## 4 12.32368 ## 5 13.20056 ## 6 12.69091 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2ORegressionMetrics: glm ## ## MSE: 0.01364287 ## RMSE: 0.1168027 ## MAE: 0.0826363 ## RMSLE: 0.009030871 ## Mean Residual Deviance : 0.01364287 ## R^2 : 0.915491 ## Null Deviance :141.57 ## Null D.o.F. :875 ## Residual Deviance :11.95116 ## Residual D.o.F. :530 ## AIC :-582.0349 # convert predicted values to non-transformed caret::RMSE(as.vector(exp(pred)), ames_test$Sale_Price) ## [1] 24147.6 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) ## [1] TRUE 3.5 Implementation: Binary Classification To illustrate various regularization concepts for a binary classification problem we will use the employee attrition data, where the goal is to predict whether or not an employee attrits (‚ÄúYes‚Äù vs. ‚ÄúNo‚Äù). The easiest way to have consistent interpretable results is to recode the response as 1 for the positive class (‚ÄúYes‚Äù) and 0 for the other class (‚ÄúNo‚Äù). attrition &lt;- rsample::attrition %&gt;% mutate(Attrition = recode(Attrition, &quot;Yes&quot; = 1, &quot;No&quot; = 0)) %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) 3.5.1 glmnet Similar to the regression application, for the classification data set we need to perfom some extra processing up-front to prepare for modeling with glmnet. First, many of the features are categorical; consequently, we need to either ordinal encode or one-hot encode these variables so that all features are numeric. Also, glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set and convert our features to a matrix. Since our response variable is imbalanced, we use rsample and strat to perform stratified sampling so that both our training and testing data sets have similar proportion of response levels. # one-hot encode our data with model.matrix one_hot &lt;- model.matrix( ~ ., attrition)[, -1] %&gt;% as.data.frame() # Create training and testing sets set.seed(123) split &lt;- initial_split(one_hot, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-Attrition) %&gt;% as.matrix() train_y &lt;- train$Attrition test_x &lt;- test %&gt;% select(-Attrition) %&gt;% as.matrix() test_y &lt;- test$Attrition # check that train &amp; test sets have common response ratio table(train_y) %&gt;% prop.table() ## train_y ## 0 1 ## 0.8385726 0.1614274 table(test_y) %&gt;% prop.table() ## test_y ## 0 1 ## 0.8395904 0.1604096 3.5.1.1 Basic implementation Similar to the regression problem, we apply a regularized classification model with glmnet::glmnet(). The primary difference is that for binary classification models we need to include family = binomial. Remember, the alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.14: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). We can also directly access the coefficients for a model using coef and tidy the output with tidy. Here, we check out the top 10 largest absolute coefficient terms when using the smallest and largest lambda values. We see that regardless of a large or small penalty parameter, working overtime and being a Sales Rep has the largest positive influence on the probability of attrition. Whereas being a Research Director and having high job involvement (among others) are the strongest influencers for reducing the probability of attrition. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 86.49535 78.81134 71.80996 65.43056 59.61789 54.32160 # small lambda results in large coefficients coef(ridge)[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 OverTimeYes 1.68 ## 2 JobInvolvementVery_High -1.42 ## 3 JobRoleSales_Representative 1.35 ## 4 (Intercept) 1.21 ## 5 BusinessTravelTravel_Frequently 1.17 ## 6 JobRoleResearch_Director -1.05 ## 7 JobInvolvementHigh -0.974 ## 8 EnvironmentSatisfactionVery_High -0.951 ## 9 JobSatisfactionVery_High -0.932 ## 10 WorkLifeBalanceBetter -0.908 ## # ... with 48 more rows # large lambda results in small coefficients coef(ridge)[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 58 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -1.65e+ 0 ## 2 JobRoleSales_Representative 2.62e-37 ## 3 OverTimeYes 1.95e-37 ## 4 JobRoleResearch_Director -1.55e-37 ## 5 MaritalStatusSingle 1.39e-37 ## 6 JobRoleManager -1.24e-37 ## 7 BusinessTravelTravel_Frequently 1.16e-37 ## 8 JobRoleLaboratory_Technician 9.16e-38 ## 9 JobRoleManufacturing_Director -8.95e-38 ## 10 JobInvolvementVery_High -7.66e-38 ## # ... with 48 more rows However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 3.5.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet. Here we perform a 10-fold CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses deviance as the loss function for binomial classification but you could adjust type.measure to ‚Äúauc‚Äù, ‚Äúmse‚Äù, ‚Äúmae‚Äù, or ‚Äúclass‚Äù (missclassification). # Apply CV Ridge regression to attrition data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Apply CV Ridge regression to attrition data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.15: 10-fold cross validation deviance for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Our plots above illustrate the 10-fold CV deviance across the \\(\\lambda\\) values. With the ridge model, we don‚Äôt see any improvement in the loss function as \\(\\lambda\\) increases but, with the lasso model, we see a slight improvement in the deviance as our penalty \\(\\lambda\\) gets larger, suggesting that a regular logistic regression model likely overfits our data. But as we constrain it further (continue to increase the penalty), our deviance starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases, with the optimal model containing between 38-48 predictor variables. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum deviance and the largest \\(\\lambda\\) value within one standard error of the minimum deviance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6483054 ridge$lambda.min # lambda for this min deviance ## [1] 0.0104184 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6788463 ridge$lambda.1se # lambda for this deviance ## [1] 0.04615997 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6511349 lasso$lambda.min # lambda for this min deviance ## [1] 0.001737893 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6737481 lasso$lambda.1se # lambda for this deviance ## [1] 0.005307275 We can assess this visually. Here we plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest deviance and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum deviance. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar minimum deviance scores; however, these plots illustrate that ridge is still using all 57 variables whereas the lasso model can get a similar deviance by reducing our feature set from 57 down to 48. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 38 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 38 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 3.16: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we‚Äôve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. lasso &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) elastic1 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.25) elastic2 &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.75) ridge &lt;- glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0.0) par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) plot(lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso (Alpha = 1)\\n\\n\\n&quot;) plot(elastic1, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .25)\\n\\n\\n&quot;) plot(elastic2, xvar = &quot;lambda&quot;, main = &quot;Elastic Net (Alpha = .75)\\n\\n\\n&quot;) plot(ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge (Alpha = 0)\\n\\n\\n&quot;) Figure 3.17: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. As we did in the regression section, we create a common fold_id, which just allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of alphas from 0-1, and empty columns where we‚Äôll dump our model results into. Use caution when including \\(\\alpha = 0\\) or \\(\\alpha = 1\\) in the grid search. \\(\\alpha = 0\\) will produce a dense solution and it can be very slow (or even impossible) to compute in large N situations. \\(\\alpha = 1\\) has no \\(\\ell_2\\) penalty, so it is therefore less numerically stable and can be very slow as well due to slower convergence. If you experience slow computation, we recommend searching across \\(\\alpha\\) values of 0.1, .25, .5, .75, .9. # maintain the same folds across all models fold_id &lt;- sample(1:10, size = length(train_y), replace=TRUE) # search across a range of alphas tuning_grid &lt;- tibble::tibble( alpha = seq(0, 1, by = .1), dev_min = NA, dev_1se = NA, lambda_min = NA, lambda_1se = NA ) Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error deviance values and their respective \\(\\lambda\\) values. This grid search took 36 seconds. # Warning - this is not fast! See H2O section for faster approach for(i in seq_along(tuning_grid$alpha)) { # fit CV model for each alpha value fit &lt;- cv.glmnet( train_x, train_y, family = &quot;binomial&quot;, alpha = tuning_grid$alpha[i], foldid = fold_id ) # extract MSE and lambda values tuning_grid$dev_min[i] &lt;- fit$cvm[fit$lambda == fit$lambda.min] tuning_grid$dev_1se[i] &lt;- fit$cvm[fit$lambda == fit$lambda.1se] tuning_grid$lambda_min[i] &lt;- fit$lambda.min tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se } tuning_grid ## # A tibble: 11 x 5 ## alpha dev_min dev_1se lambda_min lambda_1se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.656 0.699 0.00949 0.0735 ## 2 0.1 0.656 0.696 0.00685 0.0441 ## 3 0.2 0.655 0.696 0.00599 0.0320 ## 4 0.3 0.655 0.698 0.00481 0.0257 ## 5 0.4 0.655 0.699 0.00434 0.0211 ## 6 0.5 0.655 0.697 0.00381 0.0169 ## 7 0.6 0.655 0.699 0.00349 0.0155 ## 8 0.7 0.656 0.698 0.00299 0.0132 ## 9 0.8 0.656 0.697 0.00287 0.0116 ## 10 0.9 0.656 0.701 0.00255 0.0113 ## 11 1 0.656 0.700 0.00230 0.0102 If we plot the deviance ¬± one standard error for the optimal \\(\\lambda\\) value for each alpha setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model (alpha = 1) with \\(\\lambda = 0.0023\\), gain the benefits of its feature selection capability and reasonably assume no loss in accuracy. tuning_grid %&gt;% mutate(se = dev_1se - dev_min) %&gt;% ggplot(aes(alpha, dev_min)) + geom_line(size = 2) + geom_ribbon(aes(ymax = dev_min + se, ymin = dev_min - se), alpha = .25) + ggtitle(&quot;Deviance ¬± one standard error&quot;) Figure 3.18: MSE ¬± one standard error for different alpha penalty parameters. 3.5.1.3 Visual interpretation 3.5.1.3.1 Variable importance Similar to the regularized regression models, regularized classification models assume a monotonic linear relationship between the predictor variables and the response. The primary difference is in what the linear relationship means. For binary classification models, the linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the log-odds probability of the response variable. This constant change is represented by the given coefficient for a predictor. Log odds are an alternate way of expressing probabilities. The odds of a positive outcome are simply represented as \\(\\frac{prob(positive\\_outcome)}{prob(negative\\_outcome)}\\). So if there is a .2 probability of an employee attriting, the odds ratio is \\(.2 \\div .8 = .25\\). Consequently, the log odds for this employee is \\(log(.25) = -1.386294\\). An employee with 50% change of attriting has a log odds of \\(log(.5 \\div .5) = 0\\) so negative log odds means greater probability of not attriting and positive log odds means greater probability of attriting. Consequently, this makes understanding variable relationships simple with regularized models. Those variables with largest positive coefficients have the strongest influence on increasing the probability of attrition whereas those variables with the largest negative coefficients have the stongest influence on decreasing the probability of attrition. I‚Äôll illustrate with the following models. lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1) ridge &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 0) Our ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection but it will typically push most variables to near zero and allow the most influential variables to be more prominent. The following extracts the coefficients of our ridge model and plots predictor coefficients. You can see that some of the variables have coefficients closer to zero but there are also many variables that have a strong positive influence on the probability of attrition (i.e. OverTimeYes, JobRoleSales_Representative, BusinessTravelTravel_Frequently) and there are others that have a strong negative influence on the probability of attrition (i.e. JobInvolvementVery_High, JobRoleResearch_Director, JobInvolvementHigh). coef(ridge, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (ridge penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.19: A ridge penalty will retain all variables but push many coefficients to near zero. Consequently, we retain any minor signals that all features provide but those coefficients with the largest absolute values represent the most influential predictors in our model. Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those features most influential to predictive accuracy. If we select the model with the minimum deviance and plot all variables in that model we see similar results to the ridge model regarding the most influential variables. However, the lasso model has pushed 9 variables to have zero coefficients and has retained the remaining 48, effectively performing automated feature selection. coef(lasso, s = &quot;lambda.min&quot;) %&gt;% broom::tidy() %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(value, reorder(row, value), color = value &gt; 0)) + geom_point(show.legend = FALSE) + ggtitle(&quot;Influential variables (lasso penalty)&quot;) + xlab(&quot;Coefficient&quot;) + ylab(NULL) Figure 3.20: A lasso penalty will perform feature selection by pushing coefficients to zero. Consequently, we can view all coefficients to see which features were selected; however, our objective usually is still to identify those features with the strongest signal (largest absolute coefficient values). 3.5.1.3.2 ROC curve We can visualize the ROC curve with the ROCR and pROC packages. Both packages compare the predicted log-odds output (pred) to the actual observed class. ROC curves become more interesting and useful when we compare multiple models, which we will see in later chapters. If you do not include legacy.axes = TRUE in the plot() call for the pROC curve, your x-axis will be reversed. library(ROCR) library(pROC) # predict pred &lt;- predict(ridge, s = ridge$lambda.min, train_x) # plot structure par(mfrow = c(1, 2)) # ROCR plot prediction(pred, train_y) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) %&gt;% plot(main = &quot;ROCR ROC curve&quot;) #pROC plot roc(train_y, as.vector(pred)) %&gt;% plot(main = &quot;pROC ROC curve&quot;, legacy.axes = TRUE) 3.5.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Two caveats: You need to supply predict an s parameter with the preferred model‚Äôs \\(\\lambda\\) value. For example, here we create a lasso model, which provides me a minimum deviance of 0.648. We use the \\(\\lambda\\) for this model by specifying s = cv_lasso$lambda.min. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. # optimal model cv_lasso &lt;- cv.glmnet(train_x, train_y, family = &quot;binomial&quot;, alpha = 1.0) min(cv_lasso$cvm) ## [1] 0.6475805 # predict and get log-odds pred_log_odds &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x) head(pred_log_odds) ## 1 ## 2 -4.11789899 ## 3 0.18751231 ## 14 -3.26688392 ## 25 -2.17385203 ## 39 0.02202369 ## 46 -4.44305986 # predict probability pred_probs &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;response&quot;) head(pred_probs) ## 1 ## 2 0.01601793 ## 3 0.54674120 ## 14 0.03672490 ## 25 0.10212328 ## 39 0.50550570 ## 46 0.01162321 # predict and get predicted class pred_class &lt;- predict(cv_lasso, s = cv_lasso$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## 2 &quot;0&quot; ## 3 &quot;1&quot; ## 14 &quot;0&quot; ## 25 &quot;0&quot; ## 39 &quot;1&quot; ## 46 &quot;0&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the no information rate is 0.8396. This represents the ratio of non-attrition to attrition rates. The goal is to increase prediction accuracy over and above this rate. We see that our overall accuracy is 0.887. The primary weakness in our model is that for many employees that attrit, we tend to predict non-attrit. This is illustrated by our low sensitivity. The positive argument allows you to specify which value corresponds to the ‚Äúpositive‚Äù result. This will impact how you interpret certain metrics that are based on true positive and false negative results (i.e. sensitivity, specificity). caret::confusionMatrix(factor(pred_class), factor(test_y), positive = &quot;1&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 238 25 ## 1 8 22 ## ## Accuracy : 0.8874 ## 95% CI : (0.8455, 0.9212) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.013017 ## ## Kappa : 0.5102 ## Mcnemar&#39;s Test P-Value : 0.005349 ## ## Sensitivity : 0.46809 ## Specificity : 0.96748 ## Pos Pred Value : 0.73333 ## Neg Pred Value : 0.90494 ## Prevalence : 0.16041 ## Detection Rate : 0.07509 ## Detection Prevalence : 0.10239 ## Balanced Accuracy : 0.71778 ## ## &#39;Positive&#39; Class : 1 ## 3.5.2 h2o To perform regularized logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 259 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we do not need to one-hot encode or standardize our variables as h2o will do this for us. However, we do need to convert our training and test data to h2o objects. # Create training and testing sets set.seed(123) split &lt;- initial_split(attrition, prop = .8, strata = &quot;Attrition&quot;) train &lt;- training(split) test &lt;- testing(split) # convert training data to h2o object train_h2o &lt;- as.h2o(train) # convert test data to h2o object test_h2o &lt;- as.h2o(test) # set the response column to Attrition response &lt;- &quot;Attrition&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), &quot;Attrition&quot;) 3.5.2.1 Basic implementation Similar to our regression problem, we use h2o.glm to perform a regularized logistic regression model. The primary difference is that we need to set family = &quot;binomial&quot; to signal a binary classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default h2o.glm model with alpha = .5 and it performs a 10 fold cross validation (nfolds = 10). # train your model, where you specify alpha (performs 10-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;binomial&quot; ) # print the MSE and AUC for the validation data h2o.mse(h2o_fit1, xval = TRUE) ## [1] 0.09739988 h2o.auc(h2o_fit1, xval = TRUE) ## [1] 0.8345038 We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). h2o.performance(h2o_fit1, xval = TRUE) ## H2OBinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.09739988 ## RMSE: 0.3120895 ## LogLoss: 0.3347975 ## Mean Per-Class Error: 0.258244 ## AUC: 0.8345038 ## Gini: 0.6690076 ## R^2: 0.2804837 ## Residual Deviance: 788.1133 ## AIC: 914.1133 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 898 89 0.090172 =89/987 ## 1 81 109 0.426316 =81/190 ## Totals 979 198 0.144435 =170/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.351656 0.561856 156 ## 2 max f2 0.127301 0.631939 267 ## 3 max f0point5 0.497019 0.608696 105 ## 4 max accuracy 0.728176 0.875106 54 ## 5 max precision 0.989718 1.000000 0 ## 6 max recall 0.001056 1.000000 396 ## 7 max specificity 0.989718 1.000000 0 ## 8 max absolute_mcc 0.497019 0.478350 105 ## 9 max min_per_class_accuracy 0.140575 0.757852 257 ## 10 max mean_per_class_accuracy 0.234064 0.762571 209 3.5.2.2 Tuning Next, we‚Äôll use h2o.grid to perform our grid search. The results show that \\(\\alpha = .1\\) performed best; however, the improvement is marginal. This grid search took 7 seconds. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .1)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;binomial&quot;, grid_id = &quot;grid_search_glm_classification&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_classification&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_classification ## Used hyper parameters: ## - alpha ## Number of models: 11 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.1] grid_search_glm_classification_model_1 0.32729191697214505 ## 2 [0.4] grid_search_glm_classification_model_4 0.3308752559420129 ## 3 [0.2] grid_search_glm_classification_model_2 0.33248215525410824 ## 4 [0.8] grid_search_glm_classification_model_8 0.33249327956862135 ## 5 [0.7] grid_search_glm_classification_model_7 0.3348307314321876 ## 6 [0.5] grid_search_glm_classification_model_5 0.3354479575686281 ## 7 [1.0] grid_search_glm_classification_model_10 0.33563334658821187 ## 8 [0.3] grid_search_glm_classification_model_3 0.33779138839826356 ## 9 [0.0] grid_search_glm_classification_model_0 0.338497245432312 ## 10 [0.6] grid_search_glm_classification_model_6 0.33925009415446955 ## 11 [0.9] grid_search_glm_classification_model_9 0.33927152577597053 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.09538328 h2o.auc(best_model, xval = TRUE) ## [1] 0.8384072 # get optimal parameters best_model@parameters$lambda ## [1] 0.0006111815 best_model@parameters$alpha ## [1] 0.1 3.5.2.3 Visual Interpretation 3.5.2.3.1 Variable importance To identify the most influential variables we can use h2o‚Äôs variable importance plot. Recall that for a GLM model, variable importance is simply represented by the standardized coefficients. We see that JobInvolvement.Low and JobRole.Sales_Representative have the largest influence in increasing the probability of attrition whereas JobRole.Research_Director and OverTime.No have the largest influence in decreasing the probability of attrition. # get top 25 influential variables h2o.varimp_plot(best_model, num_of_features = 25) Figure 3.21: H2O‚Äôs variable importance plot. Provides the same output as plotting the standardized coefficients (h2o.std_coef_plot). To illustrate how the predicted response changes based on these influential variables, we can leverage the partial dependence inforamtion. For example, we can assess the partial dependence plots of the JobInvolvement predictor which shows up at the top of our variable importance plot. We see that mean predicted probability of attrition increases as job involvement decreases; but we can also see the significant increase in probability when an employee has a low job involvement. # partial dependence plots for top 2 influential variables h2o.partialPlot(best_model, data = train_h2o, cols = &quot;JobInvolvement&quot;, plot = FALSE) %&gt;% as.data.frame() %&gt;% mutate(JobInvolvement = factor(JobInvolvement, levels = levels(attrition$JobInvolvement))) %&gt;% ggplot(aes(JobInvolvement, mean_response)) + geom_col() + ggtitle(&quot;Average predicted probability of attrition&quot;) Figure 3.22: There is a significant increase in the predicted probability of attrition as an employee‚Äôs level of job involvement decreases to a low level. Similarly, for a continuous variable, we can assess the PDP. For example, age is one of the few continuous predictor variables in this data set and we see that our regularized logistic regression model predicts a continuously decreasing probability of attrition as employees get older. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Age&quot;) Figure 3.23: As an employee gets older, the predicted probability of attrition decreases. 3.5.2.3.2 ROC curve Earlier, we saw that this model produced a 10-fold CV AUC of .84. We can visualize this by plotting the ROC curve using the following: h2o.performance(best_model, xval = TRUE) %&gt;% plot() Figure 3.24: ROC curve for our best performing regularized H2O GLM model. 3.5.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 3 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model does a slightly better job predicting true attrition but also does slightly worse in false positives. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 ## 1 0 0.9921441 0.007855913 ## 2 1 0.4230720 0.576927982 ## 3 0 0.9724310 0.027569028 ## 4 0 0.9093784 0.090621612 ## 5 1 0.4451610 0.554838982 ## 6 0 0.9863366 0.013663436 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OBinomialMetrics: glm ## ## MSE: 0.0936628 ## RMSE: 0.3060438 ## LogLoss: 0.328802 ## Mean Per-Class Error: 0.2543678 ## AUC: 0.8418959 ## Gini: 0.6837917 ## R^2: 0.3045444 ## Residual Deviance: 192.678 ## AIC: 324.678 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 236 10 0.040650 =10/246 ## 1 22 25 0.468085 =22/47 ## Totals 258 35 0.109215 =32/293 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.499766 0.609756 34 ## 2 max f2 0.215378 0.690299 79 ## 3 max f0point5 0.600664 0.677419 26 ## 4 max accuracy 0.600664 0.890785 26 ## 5 max precision 0.991619 1.000000 0 ## 6 max recall 0.003581 1.000000 274 ## 7 max specificity 0.991619 1.000000 0 ## 8 max absolute_mcc 0.499766 0.555889 34 ## 9 max min_per_class_accuracy 0.215378 0.787234 79 ## 10 max mean_per_class_accuracy 0.215378 0.806219 79 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.6 Implementation: Multinomial Classification To illustrate various regularization concepts for a multinomial classification problem we will use the mnist data, where the goal is to predict handwritten numbers ranging from 0-9. # import mnist training and testing data train &lt;- data.table::fread(&quot;../data/mnist_train.csv&quot;, data.table = FALSE) test &lt;- data.table::fread(&quot;../data/mnist_test.csv&quot;, data.table = FALSE) 3.6.1 glmnet Since the mnist data contains all numeric predictors (darkness density ranging from 0-255), we do not need to one-hot encode our feature set. Consequently, we only need to convert our features into a matrix and seperate the response variable (V785). For a multinomial problem our response needs to be either discrete integer values or set as a factor. Since our response values are discrete integer values from 0-9 we can leave them as is. # separate features from response variable for glmnet train_x &lt;- train %&gt;% select(-V785) %&gt;% as.matrix() train_y &lt;- train$V785 test_x &lt;- test %&gt;% select(-V785) %&gt;% as.matrix() test_y &lt;- test$V785 # check the response ratios across the train &amp; test sets table(train_y) %&gt;% prop.table() %&gt;% round(2) ## train_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 table(test_y) %&gt;% prop.table() %&gt;% round(2) ## test_y ## 0 1 2 3 4 5 6 7 8 9 ## 0.10 0.11 0.10 0.10 0.10 0.09 0.10 0.10 0.10 0.10 3.6.1.1 Basic implementation To perform a regularized multinomial GLM, we simply change the family parameter to ‚Äúmultinomial‚Äù. In this example we perform a full ridge penalty (alpha = 0) model. One difference with a multinomial model is that when you plot the model results, rather than seeing only one plot with the coefficient values across the spectrum of \\(\\lambda\\) values, you will get an individual plot for the coefficients for each response category. # Apply Ridge regression to mnist data ridge &lt;- glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0 ) par(mfrow = c(2, 5)) plot(ridge, xvar = &quot;lambda&quot;) Figure 3.25: Coefficients for each multinomial response as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Also, the coefficients are stored in a list separated by the response category. We can access these coefficients in a similar manner as we did the regression and binary classification coefficients; however, we need to index for the response category of interest. For example, the following gets the coefficients for response category ‚Äú9‚Äù (note the indexing: coef(ridge)$9). # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 100] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -0.428 ## 2 V170 -0.0762 ## 3 V753 0.0505 ## 4 V703 0.0345 ## 5 V780 -0.0317 ## 6 V732 0.0264 ## 7 V726 0.0246 ## 8 V505 0.0220 ## 9 V421 -0.0147 ## 10 V225 -0.0139 ## # ... with 775 more rows # coefficients for response &quot;9&quot; with small lambda penalty coef(ridge)$`9`[, 1] %&gt;% tidy() %&gt;% arrange(desc(abs(x))) ## # A tibble: 785 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&quot; -7.12e- 3 ## 2 V753 3.18e-38 ## 3 V732 2.39e-38 ## 4 V170 -1.69e-38 ## 5 V16 -1.11e-38 ## 6 V703 8.13e-39 ## 7 V533 -6.68e-39 ## 8 V505 6.46e-39 ## 9 V33 -6.26e-39 ## 10 V752 4.64e-39 ## # ... with 775 more rows 3.6.1.2 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation with cv.glmnet as we did with the regression and binary classification examples. However, due to the magnitude of the data slowing glmnet down, rather than perform a full grid search across many alpha settings we only perform a 5-fold CV glmnet model for three alpha values: 1 (ridge), 0.5 (elastic net), and 2 (lasso). As your data set increases, glmnet begins to slow down considerably compared to h2o. Consequently, I reduced k to a 5-fold CV; however, to run a single 5-fold CV on this training set still took 109 minutes! If you parallelize the process with parallel = TRUE (which requires you to use doMC or some other parallelizer) you can achieve some speed improvements. # parallelize the process library(doMC) registerDoMC(cores = 4) # Apply CV Ridge regression to mnist data ridge &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 0, nfolds = 5, parallel = TRUE ) # Apply CV elastic net regression to mnist data elastic &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = .5, nfolds = 5, parallel = TRUE ) # Apply CV lasso regression to mnist data lasso &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;multinomial&quot;, alpha = 1, nfolds = 5, parallel = TRUE ) # plot results par(mfrow = c(1, 3)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(elastic, main = &quot;Elastic net penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 3.26: 5-fold cross validation deviance for a ridge, lasso, and elastic net model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest deviance and the second represents the \\(\\lambda\\) with a deviance within one standard error of the minimum deviance. Note how the top of the ridge penalty deviance plot indicates only 717 of the 784 predictors are in the model. The reason for this is not because the ridge model pushed 67 coefficients to zero because a ridge model does not do that. Rather, there are 67 predictors that have zero variance so they are not included in the model (see this with preProcess(train_x, ‚Äúzv‚Äù)). In the performance plots in Figure 3.26 it is difficult to see if the models differ in their minimum error rate (multinomial deviance). But if we look at the minimum deviance (and the largest deviance within one standard error) in the below code chunk, we see that the elastic net provides the optimal performance. Consequently, it appears we could use between 250-300 of the 784 predictors and still achieve optimal performance. # Ridge model min(ridge$cvm) # minimum deviance ## [1] 0.6194693 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min deviance ## [1] 0.6194693 # Elastic net model min(elastic$cvm) # minimum deviance ## [1] 0.5627915 elastic$cvm[elastic$lambda == elastic$lambda.1se] # 1 st.error of min deviance ## [1] 0.5682305 # Lasso model min(lasso$cvm) # minimum deviance ## [1] 0.6557272 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min deviance ## [1] 0.6557272 So the elastic net model is minimizing the multinomial deviance loss function, but how does this translate to the accuracy of this model? We can assess the confusion matrix for this data on the training data. At first glance, the confusion matrix is difficult to discern differences. However, looking at the class statistics and specifically the sensitivity and specificity we can extract some useful insights. First, the specificity is 0.99 across all numbers indicating that the model does well in classifying non-events (basically, no number has a significantly higher false positive rate than the other numbers). Second, looking at the sensitivity, we can see that our model does the best at accurately predicting the numbers 0, 1, and 6. However, it does worst at accurately predicting the numbers 2, 3, 5, 8, and 9. The number 8 has the lowest sensitivity rate and when we look at the confusion matrix we can see that our model often classifies the number 8 as the numbers 5, 3, and 1. pred_class &lt;- predict(elastic, s = elastic$lambda.min, train_x, type = &quot;class&quot;) caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 5780 1 26 16 11 43 27 10 28 19 ## 1 1 6580 48 28 27 20 15 25 115 25 ## 2 16 33 5452 126 24 38 33 59 56 14 ## 3 8 15 84 5547 9 145 0 19 125 78 ## 4 10 6 65 8 5506 50 30 47 26 135 ## 5 27 23 19 181 9 4882 69 10 129 36 ## 6 34 3 57 17 45 80 5714 3 36 2 ## 7 5 13 67 47 13 16 2 5911 15 157 ## 8 36 58 120 114 31 109 26 15 5251 41 ## 9 6 10 20 47 167 38 2 166 70 5442 ## ## Overall Statistics ## ## Accuracy : 0.9344 ## 95% CI : (0.9324, 0.9364) ## No Information Rate : 0.1124 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9271 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.97586 0.9760 0.91507 0.90475 0.94249 0.90057 0.96553 0.94350 0.89745 0.91478 ## Specificity 0.99665 0.9943 0.99262 0.99103 0.99304 0.99078 0.99488 0.99377 0.98984 0.99027 ## Pos Pred Value 0.96964 0.9558 0.93181 0.91990 0.93592 0.90659 0.95376 0.94637 0.90519 0.91186 ## Neg Pred Value 0.99735 0.9970 0.99066 0.98918 0.99379 0.99013 0.99622 0.99341 0.98893 0.99062 ## Prevalence 0.09872 0.1124 0.09930 0.10218 0.09737 0.09035 0.09863 0.10442 0.09752 0.09915 ## Detection Rate 0.09633 0.1097 0.09087 0.09245 0.09177 0.08137 0.09523 0.09852 0.08752 0.09070 ## Detection Prevalence 0.09935 0.1147 0.09752 0.10050 0.09805 0.08975 0.09985 0.10410 0.09668 0.09947 ## Balanced Accuracy 0.98625 0.9851 0.95384 0.94789 0.96776 0.94568 0.98020 0.96863 0.94365 0.95252 3.6.1.3 Visual interpretation Interpreting the underlying predictor mechanisms with a multinomial problem is much like the regression and binary classification problems but with a few extra nuances. The first thing to remember is that although we started with 784 predictors, our full ridge model only used 717 because 67 predictors had zero variance. This is not unique to multinomial problems but its important to understand which variables these are because, in an organizational situation, we can assess whether or not we should continue collecting this information. In this example, they primarily represent pixels along the very edge of the images. # zero variance predictor variables that offer no potential signal names(which(sapply(train, var) == 0)) ## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; &quot;V5&quot; &quot;V6&quot; &quot;V7&quot; &quot;V8&quot; &quot;V9&quot; &quot;V10&quot; &quot;V11&quot; ## [12] &quot;V12&quot; &quot;V17&quot; &quot;V18&quot; &quot;V19&quot; &quot;V20&quot; &quot;V21&quot; &quot;V22&quot; &quot;V23&quot; &quot;V24&quot; &quot;V25&quot; &quot;V26&quot; ## [23] &quot;V27&quot; &quot;V28&quot; &quot;V29&quot; &quot;V30&quot; &quot;V31&quot; &quot;V32&quot; &quot;V53&quot; &quot;V54&quot; &quot;V55&quot; &quot;V56&quot; &quot;V57&quot; ## [34] &quot;V58&quot; &quot;V83&quot; &quot;V84&quot; &quot;V85&quot; &quot;V86&quot; &quot;V112&quot; &quot;V113&quot; &quot;V141&quot; &quot;V142&quot; &quot;V169&quot; &quot;V477&quot; ## [45] &quot;V561&quot; &quot;V645&quot; &quot;V646&quot; &quot;V672&quot; &quot;V673&quot; &quot;V674&quot; &quot;V700&quot; &quot;V701&quot; &quot;V702&quot; &quot;V728&quot; &quot;V729&quot; ## [56] &quot;V730&quot; &quot;V731&quot; &quot;V755&quot; &quot;V756&quot; &quot;V757&quot; &quot;V758&quot; &quot;V759&quot; &quot;V760&quot; &quot;V781&quot; &quot;V782&quot; &quot;V783&quot; ## [67] &quot;V784&quot; However, in a multinomial problem, each response category will not always use all the predictors that have variance. In fact, the below code shows that the number zero only uses 189 predictors whereas the number 3 uses 296. You can set type.multinomial = ‚Äúgrouped‚Äù within cv.glmnet() to force all predictors to be in or out together. coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% count(response) ## # A tibble: 10 x 2 ## Response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 189 ## 2 1 206 ## 3 2 281 ## 4 3 296 ## 5 4 272 ## 6 5 264 ## 7 6 267 ## 8 7 278 ## 9 8 228 ## 10 9 283 Moreover, each response category will use the predictors in different ways. Similar to the binary classification problem, the coefficients represent the change in log-odds probability of the response variable for a one unit change in the predictor. For example, below we see that for the response category ‚Äú0‚Äù, a one unit increase in the darkness of the pixel represented by feature V41 causes a 0.0089 log odds increase that the response will be the number ‚Äú0‚Äù. A 0.0089 log odds increase translates to \\(\\frac{e^{0.008912499}}{1+e^{0.008912499}} = 0.5022281\\) probability increase. The below code chunk extracts the coefficients for each response category, combines them all into a single data frame, and removes the intercept (since we just care about the predictor coefficients). # tidy up the coefficients for downstream assessment vi &lt;- coef(elastic) %&gt;% purrr::map(tidy) %&gt;% bind_rows(.id = &quot;response&quot;) %&gt;% filter(row != &quot;(Intercept)&quot;) %&gt;% select(response, feature = row, coefficient = value) head(vi) ## response feature coefficient ## 1 0 V41 8.912499e-03 ## 2 0 V45 8.816906e-03 ## 3 0 V60 8.381852e-02 ## 4 0 V72 -2.083206e-04 ## 5 0 V106 -1.630429e-03 ## 6 0 V124 1.720361e-05 Similar to the binary classification problem, one of our main concerns is to understand which predictors have the largest influence on each response category. The following identifies the top 10 predictors with the largest absolute coefficients. These predictors represent those that have the largest impact to the probability of that response. For example, as features V60, V225, and V504 increase (those pixels become darker), they have the largest increase in the probability that the response will be ‚Äú0‚Äù. Alternatively, as features V719 and V363 increase, they have the largest decrease in the probability that the response will be ‚Äú0‚Äù. Since a given predictor can have a different coefficient for each response category, each response category can have their own unique variable importance list. vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, feature, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.27: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. The above plot identifies thse variables most influential for each given response category. However, we also want to understand which variables are influential across all, or most, of the responses. The below identifies three predictors (V375, V515, V572) that have non-zero coefficients for all the response categories. Plotting these variables allows us to see how the strength and direction of the signal varies across the response categories. These features represent pixels in the images that are used by each handwritten number. # predictors that are influential across all or many of the responses vi %&gt;% count(feature) %&gt;% arrange(desc(n)) ## # A tibble: 653 x 2 ## feature n ## &lt;chr&gt; &lt;int&gt; ## 1 V375 10 ## 2 V515 10 ## 3 V572 10 ## 4 V402 9 ## 5 V239 8 ## 6 V268 8 ## 7 V301 8 ## 8 V324 8 ## 9 V326 8 ## 10 V353 8 ## # ... with 643 more rows vi %&gt;% filter(feature %in% c(&quot;V375&quot;, &quot;V515&quot;, &quot;V572&quot;)) %&gt;% ggplot(aes(coefficient, feature, fill = coefficient &gt; 0)) + geom_col() + coord_flip() + facet_wrap(~ response, ncol = 5) Figure 3.28: Three variables provide signals for all the response categories; however, the strength and direction of the signal can vary across the responses. Similarly, certain predictors may only provide a signal for only one response category. We also want to assess these as they provide a unique signal for a particular response. For example, feature V170 is only influential for response ‚Äú3‚Äù and increases the probability by 0.525. # only 82 predictors that play a role in just one response singles &lt;- vi %&gt;% count(feature) %&gt;% filter(n == 1) %&gt;% .$feature vi %&gt;% filter(feature %in% singles) %&gt;% arrange(desc(abs(coefficient))) %&gt;% slice(1:10) %&gt;% mutate(prob = exp(coefficient) / (1 + exp(coefficient))) ## response feature coefficient prob ## 1 3 V170 0.10146013 0.5253433 ## 2 0 V60 0.08381852 0.5209424 ## 3 0 V225 0.07995005 0.5199769 ## 4 7 V780 0.05972853 0.5149277 ## 5 7 V534 0.03223708 0.5080586 ## 6 7 V778 0.02278233 0.5056953 ## 7 2 V392 0.02252209 0.5056303 ## 8 2 V82 0.02225574 0.5055637 ## 9 8 V335 0.01925485 0.5048136 ## 10 6 V88 0.01859690 0.5046491 Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. Similar to the binary classification problem: You need to supply predict an s parameter with the preferred model‚Äôs \\(\\lambda\\) value. The default predicted values are the log odds. If you want the probability, include type = &quot;response&quot;. If you want to predict the categorical response, include type = &quot;class&quot;. Additionally, the predicted output is in the form of an array. Consequently, to assess the predicted values for the first five observations across all response categories, index with the following: # predict and get log-odds pred_log_odds &lt;- predict(elastic, s = elastic$lambda.min, test_x) pred_log_odds[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] -5.432659 -1.339061 -2.333135 0.7305298 -3.606102 0.5498658 -6.628586 -6.8034856 6.3175820 0.353073 ## [2,] -8.302420 -5.464651 -3.410668 7.5492386 -4.806773 3.3335312 -4.941261 -0.1618101 0.7612956 2.373736 ## [3,] 7.266914 -8.224763 1.052633 1.6484586 -8.055438 -9.2497608 2.820177 -12.5006341 8.8967193 -10.473515 ## [4,] 8.415969 -10.700918 1.233072 -5.6888200 -2.364087 -0.6202491 5.928128 -7.3944607 0.2065292 -5.634506 ## [5,] -6.395731 7.228976 2.144250 -0.1299773 -4.381965 -2.0272380 -2.086397 -1.8958485 1.1838882 -2.138867 # predict probability pred_probs &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;response&quot;) pred_probs[1:5, , 1] ## 0 1 2 3 4 5 6 7 8 9 ## [1,] 7.808164e-06 4.681405e-04 0.0001732428 3.708412e-03 4.850791e-05 3.095470e-03 2.361373e-06 1.982470e-06 0.9899515761 2.542498e-03 ## [2,] 1.277183e-07 2.181128e-06 0.0000170104 9.784562e-01 4.211081e-06 1.444385e-02 3.681174e-06 4.382031e-04 0.0011029991 5.531581e-03 ## [3,] 1.633925e-01 3.056908e-08 0.0003268920 5.931549e-04 3.620925e-08 1.096811e-08 1.914428e-03 4.249083e-10 0.8337729101 3.225981e-09 ## [4,] 9.222861e-01 4.597347e-09 0.0007004437 6.906115e-07 1.919319e-05 1.097706e-04 7.663202e-02 1.254537e-07 0.0002509293 7.291585e-07 ## [5,] 1.198730e-06 9.905039e-01 0.0061317946 6.308170e-04 8.980257e-06 9.460923e-05 8.917459e-05 1.078935e-04 0.0023469721 8.461622e-05 # predict and get predicted class pred_class &lt;- predict(elastic, s = elastic$lambda.min, test_x, type = &quot;class&quot;) head(pred_class) ## 1 ## [1,] &quot;8&quot; ## [2,] &quot;3&quot; ## [3,] &quot;8&quot; ## [4,] &quot;0&quot; ## [5,] &quot;1&quot; ## [6,] &quot;5&quot; Lastly, to assess various performance metrics on our test data we can use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. We can see that the overall accuracy rate is 0.9281 and our model does well predicting ‚Äú0‚Äù, ‚Äú1‚Äù, and ‚Äú6‚Äù but poorly predicting ‚Äú2‚Äù, ‚Äú3‚Äù, ‚Äú5‚Äù, ‚Äú8‚Äù, and ‚Äú9‚Äù. caret::confusionMatrix(factor(pred_class), factor(test_y)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 959 0 7 3 1 8 10 2 6 10 ## 1 0 1114 8 1 1 2 3 9 7 8 ## 2 1 2 929 16 4 2 4 22 6 1 ## 3 1 2 15 921 1 33 2 5 21 10 ## 4 0 0 7 0 919 11 7 5 10 25 ## 5 7 1 3 29 0 779 14 0 24 5 ## 6 6 4 12 2 12 14 913 0 9 0 ## 7 5 2 11 10 6 8 3 954 10 22 ## 8 1 10 36 20 8 31 2 1 871 6 ## 9 0 0 4 8 30 4 0 30 10 922 ## ## Overall Statistics ## ## Accuracy : 0.9281 ## 95% CI : (0.9229, 0.9331) ## No Information Rate : 0.1135 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9201 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 0.9786 0.9815 0.9002 0.9119 0.9358 0.8733 0.9530 0.9280 0.8943 0.9138 ## Specificity 0.9948 0.9956 0.9935 0.9900 0.9928 0.9909 0.9935 0.9914 0.9873 0.9904 ## Pos Pred Value 0.9533 0.9662 0.9412 0.9110 0.9339 0.9037 0.9393 0.9253 0.8834 0.9147 ## Neg Pred Value 0.9977 0.9976 0.9886 0.9901 0.9930 0.9876 0.9950 0.9917 0.9886 0.9903 ## Prevalence 0.0980 0.1135 0.1032 0.1010 0.0982 0.0892 0.0958 0.1028 0.0974 0.1009 ## Detection Rate 0.0959 0.1114 0.0929 0.0921 0.0919 0.0779 0.0913 0.0954 0.0871 0.0922 ## Detection Prevalence 0.1006 0.1153 0.0987 0.1011 0.0984 0.0862 0.0972 0.1031 0.0986 0.1008 ## Balanced Accuracy 0.9867 0.9885 0.9469 0.9509 0.9643 0.9321 0.9733 0.9597 0.9408 0.9521 3.6.2 h2o To perform regularized multinomial logistic regression with h2o, we first need to initiate our h2o session. h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 820 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_fmw129 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Since our data is already split into a training test set, to prepare for modeling, we just need to convert our training and test data to h2o objects and identify the response and predictor variables. One key difference compared to prior classification procedures, to perform a multinomial modeling with h2o we need to convert the response variable to a factor. # convert training data to h2o object train_h2o &lt;- train %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # convert test data to h2o object test_h2o &lt;- test %&gt;% mutate(V785 = factor(V785)) %&gt;% as.h2o() # set the response column to V785 response &lt;- &quot;V785&quot; # set the predictor names predictors &lt;- setdiff(colnames(train), response) 3.6.2.1 Basic implementation Similar to our regression and binary classification problem, we use h2o.glm to perform a regularized multinomial logistic regression model. The primary difference is that we need to set family = &quot;multinomial&quot; to signal a multinomial classification problem. As before, by default, h2o.glm performs an elastic net model with alpha = .5 and will perform an automated search across internally generated lambda values. The following performs a default multinomial h2o.glm model with alpha = .5 and it performs a 5 fold cross validation (nfolds = 10). When you are working with large data sets, h2o provides a far more efficient approach for regularized models. Whereas glmnet took over an hour to perform a 5-fold cross validated regularized model on the mnist data, h2o took less than 2.5 minutes! # train your model, where you specify alpha (performs 5-fold CV) h2o_fit1 &lt;- h2o.glm( x = predictors, y = response, training_frame = train_h2o, nfolds = 5, keep_cross_validation_predictions = TRUE, alpha = .5, family = &quot;multinomial&quot; ) We can check out the cross-validated performance results of our model with h2o.performance. You can also get more results information using summary(h2o_fit1). One unique output worth discussing is the ‚ÄúTop-10 Hit Ratios‚Äù table. The first line of this table (k = 1, hit_ratio = 0.9213) represents the mean accuracy of our model across the 5-fold validated set. However, the second line tells us that when we take those missed predictions and use the second highest predicted probability we get a mean accuracy of 96.85% (we get an additional \\((0.9685 - 0.9213) \\times 60000 = 2832\\) observations correct.) After only 4 reapplications we are able to achieve 99% accuracy. h2o.performance(h2o_fit1, xval = TRUE) ## H2OMultinomialMetrics: glm ## ** Reported on cross-validation data. ** ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## Cross-Validation Set Metrics: ## ===================== ## ## Extract cross-validation frame with `h2o.getFrame(&quot;filea18f736ccac9_sid_8725_4&quot;)` ## MSE: (Extract with `h2o.mse`) 0.07378137 ## RMSE: (Extract with `h2o.rmse`) 0.2716273 ## Logloss: (Extract with `h2o.logloss`) 0.2843358 ## Mean Per-Class Error: 0.07972688 ## Null Deviance: (Extract with `h2o.nulldeviance`) 276156.9 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 34193.91 ## R^2: (Extract with `h2o.r2`) 0.9911615 ## AIC: (Extract with `h2o.aic`) NaN ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.921300 ## 2 2 0.968500 ## 3 3 0.984300 ## 4 4 0.991517 ## 5 5 0.995133 ## 6 6 0.997200 ## 7 7 0.998400 ## 8 8 0.999183 ## 9 9 0.999800 ## 10 10 1.000000 3.6.2.2 Tuning Since h2o is much faster than glmnet, we can perform a grid search across a wider range of alpha parameters. Here, I assess alphas equal to 0, 0.25, 0.5, 0.75, and 1 using h2o.grid to perform our grid search. The results show that \\(\\alpha = .25\\) performed best. The results also show that 4 out of 5 models have minor differences in the Log Loss loss function; however, \\(\\alpha = .0\\) (full ridge penalty) definitely performs the worst. This grid search took 20 minutes. # create hyperparameter grid hyper_params &lt;- list(alpha = seq(0, 1, by = .25)) # perform grid search grid &lt;- h2o.grid( x = predictors, y = response, training_frame = train_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE, algorithm = &quot;glm&quot;, family = &quot;multinomial&quot;, grid_id = &quot;grid_search_glm_multinomial&quot;, hyper_params = hyper_params ) # Sort the grid models by MSE sorted_grid &lt;- h2o.getGrid(&quot;grid_search_glm_multinomial&quot;, sort_by = &quot;logloss&quot;, decreasing = FALSE) sorted_grid ## H2O Grid Details ## ================ ## ## Grid ID: grid_search_glm_multinomial ## Used hyper parameters: ## - alpha ## Number of models: 5 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## alpha model_ids logloss ## 1 [0.25] grid_search_glm_multinomial_model_1 0.2826176758945207 ## 2 [1.0] grid_search_glm_multinomial_model_4 0.2833413952701668 ## 3 [0.75] grid_search_glm_multinomial_model_3 0.2834683548256082 ## 4 [0.5] grid_search_glm_multinomial_model_2 0.2855257887768779 ## 5 [0.0] grid_search_glm_multinomial_model_0 0.31518212134903734 We can check out more details of the best performing model. Our AUC (.84) is no better than our default cross-validated model. We can also extract other model parameters, which show the optimal \\(\\lambda\\) value for our model was 0.0006. # grab top model id best_h2o_model &lt;- sorted_grid@model_ids[[1]] best_model &lt;- h2o.getModel(best_h2o_model) # assess performance h2o.mse(best_model, xval = TRUE) ## [1] 0.0733229 h2o.rmse(best_model, xval = TRUE) ## [1] 0.270782 # get optimal parameters best_model@parameters$lambda ## [1] 0.000673668 best_model@parameters$alpha ## [1] 0.25 We can also assess the confusion matrix for our optimal model. Our overall accuracy is 0.9146 (\\(1-0.0854\\)), which is slightly less than the mean 5-fold CV accuracy produced by the glmnet model (0.9344). Similar to our glmnet results, our optimal model is doing a good job predicting ‚Äú0‚Äù, ‚Äú1‚Äù, and ‚Äú6‚Äù, but is poorly predicting ‚Äú2‚Äù, ‚Äú3‚Äù, ‚Äú5‚Äù, and ‚Äú8‚Äù. h2o.confusionMatrix(best_model) ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 5776 1 14 9 10 28 36 6 38 5 0.0248 = 147 / 5,923 ## 1 1 6581 32 15 6 26 3 14 55 9 0.0239 = 161 / 6,742 ## 2 28 53 5449 85 64 16 57 67 121 18 0.0854 = 509 / 5,958 ## 3 18 31 119 5553 7 180 16 49 109 49 0.0943 = 578 / 6,131 ## 4 11 34 24 8 5504 7 44 12 30 168 0.0579 = 338 / 5,842 ## 5 50 32 31 146 52 4855 78 17 121 39 0.1044 = 566 / 5,421 ## 6 31 18 31 0 35 62 5710 4 25 2 0.0351 = 208 / 5,918 ## 7 11 35 58 16 46 8 4 5905 12 170 0.0575 = 360 / 6,265 ## 8 33 151 51 128 26 134 34 15 5211 68 0.1094 = 640 / 5,851 ## 9 25 27 11 76 142 30 3 156 38 5441 0.0854 = 508 / 5,949 ## Totals 5984 6963 5820 6036 5892 5346 5985 6245 5760 5969 0.0669 = 4,015 / 60,000 3.6.2.3 Visual Interpretation Similar to glmnet, we can extract useful information from our coefficients to interpret influential variables in our predictors. First, we need to do a little clean up of our coefficient table. The coefficient table provides both the coefficient estimate and the standard error; however, what follows only focuses on the coefficient estimate. # clean up coefficient information vi &lt;- best_model@model$coefficients_table %&gt;% as.data.frame() %&gt;% tidyr::gather(response, coefficient, -names) %&gt;% filter( names != &quot;Intercept&quot;, !(stringr::str_detect(response, &quot;std&quot;))) %&gt;% mutate(response = stringr::str_remove(response, &quot;coefs_class_&quot;)) head(vi) ## names response coefficient ## 1 V13 0 1.637796e-04 ## 2 V14 0 2.447977e-05 ## 3 V15 0 1.202897e-04 ## 4 V16 0 2.886954e-03 ## 5 V33 0 1.270802e-03 ## 6 V34 0 3.842376e-04 Unlike glmnet, h2o forces all predictors to be either in or out across all response categories (there are options to remove some of the collinear columns; however, this will still produce a consistent number of predictors across all response categories). Consequently, we see that all 717 features are present for each response. count(vi, response) ## # A tibble: 10 x 2 ## response n ## &lt;chr&gt; &lt;int&gt; ## 1 0 717 ## 2 1 717 ## 3 2 717 ## 4 3 717 ## 5 4 717 ## 6 5 717 ## 7 6 717 ## 8 7 717 ## 9 8 717 ## 10 9 717 Similar to glment, we can use this information to identify the top 10 predictors with the largest absolute coefficients for each response category. These predictors represent those that have the largest impact to the probability of that response. Comparing to the glmnet variable importance plots earlier, features V60, V225, and V504 are the top 3 features that have the largest increase in the probability that the response will be ‚Äú0‚Äù (although their ordering differs slightly). However, the features that have the largest decrease in the probability of response ‚Äú0‚Äù differ from before. This may be a result of the different penalty parameter that this optimal model has compared to what we used with glmnet (\\(\\alpha=0.25\\) versus \\(\\alpha=0.5\\)). vi %&gt;% group_by(response) %&gt;% top_n(10, wt = abs(coefficient)) %&gt;% mutate(predictor = paste(response, names, sep = &quot;: &quot;)) %&gt;% ggplot(aes(coefficient, reorder(predictor, coefficient), color = coefficient &gt; 0)) + geom_point(show.legend = FALSE) + facet_wrap(~ response, scales = &quot;free_y&quot;, ncol = 5) + ylab(NULL) Figure 3.29: Top 10 most influential predictors for each multinomial response. Predictors with coefficients to the right of zero (blue) increase the probability of that response whereas predictors with coefficients to the left of zero (red) decrease the probability of that response. Remember that these linear models assume a monotonic linear relationship between the predictors and the response; meaning that for a given response category, the relationship represented by the coefficient is constant. Therefore, global and local model interpreation will be the same. See more in the machine learning interpretability chapter. 3.6.2.4 Predicting Lastly, we can use h2o.predict and h2o.performance to predict and evaluate our models performance on our hold out test data. Note how the h2o.predict function provides 11 columns - the predicted class and the probability of each class. We also produce our test set performance results with h2o.performance. If you compare the confusion matrix with the one produced by glmnet you will notice they produce very similar results. The h2o model has an overall accuracy of 0.9254 (\\(1-0.0746\\)) whereas the glmnet model achieved an accuracy of 0.9281 on the test set. And both models have about the same level of accuracy across individual response categories. # make predictions pred &lt;- h2o.predict(object = best_model, newdata = test_h2o) head(pred) ## predict p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 ## 1 8 1.202141e-05 5.972906e-04 1.974938e-04 3.407596e-03 7.913304e-05 2.536697e-03 1.125671e-05 5.718130e-06 0.9905660926 2.586700e-03 ## 2 3 3.049209e-07 2.484003e-05 5.767582e-06 9.805161e-01 6.366074e-06 1.372732e-02 8.326104e-06 4.850814e-04 0.0009518292 4.274033e-03 ## 3 8 4.521685e-01 1.335448e-07 6.741594e-04 7.897412e-05 1.444127e-07 8.704872e-07 3.896986e-03 5.145982e-09 0.5431802637 3.238420e-09 ## 4 0 8.901589e-01 4.027749e-07 9.102679e-04 5.754058e-07 3.002048e-05 1.979771e-04 1.081991e-01 1.176814e-06 0.0005003709 1.131257e-06 ## 5 1 7.789492e-06 9.859295e-01 9.676005e-03 8.276910e-04 1.749899e-05 1.464323e-04 2.368448e-04 1.720208e-04 0.0029125347 7.370373e-05 ## 6 5 1.597256e-06 2.661786e-08 1.713028e-08 8.928673e-11 4.543192e-05 9.399342e-01 1.337817e-06 3.112696e-07 0.0600170128 2.307191e-08 # assess performance h2o.performance(best_model, newdata = test_h2o) ## H2OMultinomialMetrics: glm ## ## Test Set Metrics: ## ===================== ## ## MSE: (Extract with `h2o.mse`) 0.06887707 ## RMSE: (Extract with `h2o.rmse`) 0.2624444 ## Logloss: (Extract with `h2o.logloss`) 0.2682622 ## Mean Per-Class Error: 0.07570879 ## Null Deviance: (Extract with `h2o.nulldeviance`) 46020.38 ## Residual Deviance: (Extract with `h2o.residual_deviance`) 5365.243 ## R^2: (Extract with `h2o.r2`) 0.9917859 ## AIC: (Extract with `h2o.aic`) NaN ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error Rate ## 0 963 0 0 1 0 5 5 4 2 0 0.0173 = 17 / 980 ## 1 0 1112 2 2 0 1 4 2 12 0 0.0203 = 23 / 1,135 ## 2 7 9 926 17 9 3 13 9 36 3 0.1027 = 106 / 1,032 ## 3 4 1 17 921 1 24 2 11 23 6 0.0881 = 89 / 1,010 ## 4 1 3 3 2 919 0 14 3 7 30 0.0642 = 63 / 982 ## 5 8 2 1 33 10 774 17 12 31 4 0.1323 = 118 / 892 ## 6 11 3 6 1 7 15 910 3 2 0 0.0501 = 48 / 958 ## 7 2 9 22 6 6 0 0 951 1 31 0.0749 = 77 / 1,028 ## 8 7 12 6 19 11 24 11 12 861 11 0.1160 = 113 / 974 ## 9 10 8 1 10 28 5 0 24 6 917 0.0912 = 92 / 1,009 ## Totals 1013 1159 984 1012 991 851 976 1031 981 1002 0.0746 = 746 / 10,000 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.925400 ## 2 2 0.970200 ## 3 3 0.983900 ## 4 4 0.991300 ## 5 5 0.995600 ## 6 6 0.997400 ## 7 7 0.998600 ## 8 8 0.999300 ## 9 9 0.999700 ## 10 10 1.000000 # shutdown h2o h2o.removeAll() ## [1] 0 h2o.shutdown(prompt = FALSE) 3.7 Learning More This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric (i.e. Cox proportional hazard, poisson, support vector machines) and non-parametric (i.e. Least Angle Regression, the Bayesian Lasso, neural networks) models. The following are great resources to learn more (listed in order of complexity): Applied Predictive Modeling Practical Machine Learning with H2o Introduction to Statistical Learning The Elements of Statistical Learning Statistical Learning with Sparsity References "],
["random-forest.html", "Chapter 4 Random Forest 4.1 Prerequisites 4.2 Advantages &amp; Disadvantages 4.3 The Idea 4.4 Implementation: Regression 4.5 Implementation: Binary Classification 4.6 Implementation: Multinomial Classification 4.7 Learning More", " Chapter 4 Random Forest Random forests are a modification of decision trees and bagging that builds a large collection of de-correlated trees to reduce overfitting (aka variance). They have become a very popular ‚Äúout-of-the-box‚Äù learning algorithm that enjoys good predictive performance and easy hyperparameter tuning. Many modern implementations of random forests algorithms exist; however, Leo Breiman‚Äôs algorithm3 has largely become the authoritative procedure. This chapter will cover the fundamentals of random forests. 4.1 Prerequisites Any tutorial on random forests (RF) should also include a review of decision trees, as these are models that are ensembled together to create the random forest model ‚Äì or put another way, the ‚Äútrees that comprise the forest.‚Äù Much of the complexity and detail of the random forest algorithm occurs within the individual decision trees and therefore it‚Äôs important to understand decision trees to understand the RF algorithm as a whole. Therefore, before proceeding, it is recommended that you read through http://uc-r.github.io/regression_trees prior to continuing. This chapter leverages the following packages. Some of these packages play a supporting role; however, the emphasis is on how to implement random forests with the ranger (Wright and Ziegler 2017) and h2o packages. library(rsample) # data splitting library(ranger) # a fast c++ implementation of the random forest algorithm library(h2o) # a java-based platform library(vip) # visualize feature importance library(pdp) # visualize feature effects library(ggplot2) # supports visualization library(dplyr) # basic data transformation 4.2 Advantages &amp; Disadvantages Advantages: Typically have very good performance. Remarkably good ‚Äúout-of-the box‚Äù - very little tuning required. Built-in validation set - don‚Äôt need to sacrifice data for extra validation. Does not overfit. No data pre-processing required - often works great with categorical and numerical values as is. Robust to outliers. Handles missing data - imputation not required. Provide automatic feature selection. Disadvantages: Can become slow on large data sets. Although accurate, often cannot compete with the accuracy of advanced boosting algorithms. Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.). 4.3 The Idea Random forests are built on the same fundamental principles as decision trees and bagging (check out this tutorial if you need a refresher on these techniques). Bagging trees introduces a random component in to the tree building process that reduces the variance of a single tree‚Äôs prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. For example, if we create six decision trees with different bootstrapped samples of the Boston housing data (Harrison Jr and Rubinfeld 1978), we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both lstat and rm variables driving the first few splits. Figure 4.1: Six decision trees based on different bootstrap samples. This characteristic is known as tree correlation and prevents bagging from optimally reducing variance of the predictive values. In order to reduce variance further, we need to minimize the amount of correlation between the trees. This can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways: Bootstrap: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and somewhat decorrelates them. Split-variable randomization: each time a split is to be performed, the search for the split variable is limited to a random subset of m of the p variables. Typical default values are \\(m = \\frac{p}{3}\\) (regression trees) and \\(m = \\sqrt{p}\\) (classification trees) but this should be considered a tuning parameter. When \\(m = p\\), the randomization amounts to using only step 1 and is the same as bagging. The basic algorithm for a regression or classification random forest can be generalized to the following: 1. Given training data set 2. Select number of trees to build (ntrees) 3. for i = 1 to ntrees do 4. | Generate a bootstrap sample of the original data 5. | Grow a regression or classification tree to the bootstrapped data 6. | for each split do 7. | | Select m variables at random from all p variables 8. | | Pick the best variable/split-point among the m 9. | | Split the node into two child nodes 10. | end 11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune) 12. end Since the algorithm randomly selects a bootstrap sample to train on and predictors to use at each split, tree correlation will be lessened beyond bagged trees. 4.3.1 OOB error vs. test set error Similar to bagging, a natural benefit of the bootstrap resampling process is that random forests have an out-of-bag (OOB) sample that provides an efficient and reasonable approximation of the test error. This provides a built-in validation set without any extra work on your part, and you do not need to sacrifice any of your training data to use for validation. This makes identifying the number of trees required to stablize the error rate during tuning more efficient; however, as illustrated below some difference between the OOB error and test error are expected. Figure 2.1: Random forest out-of-bag error versus validation error. Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, you‚Äôd want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation. 4.3.2 Tuning Random forests are fairly easy to tune since there are only a handful of tuning parameters. Typically, the primary concern when starting out is tuning the number of candidate variables to select from at each split. However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present: Number of trees_: We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets. Number of variables to randomly sample as candidates at each split (often referred to as mtry): When mtry \\(=p\\) the model equates to bagging. When mtry \\(=1\\) the split variable is completely random, so all variables get a chance but can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to p. Sample size: the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range. Node size: minimum number of samples within the terminal nodes. Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and a larger node size results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data). Number of terminal nodes: Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees. 4.3.3 Package implementation There are over 20 random forest packages in R.4 The oldest and most well known implementation of the Random Forest algorithm in R is the randomForest package. randomForest is not a recommended package because as your data sets grow in size randomForest does not scale well (although you can parallelize with foreach). Instead, we recommend you use the ranger and h2o packages. Since randomForest does not scale well to many of the data set sizes that organizations analyze, we will demonstrate how to implement the random forest algorithm with two fast, efficient, and highly recommended packages: ranger: a C++ implementation of Brieman‚Äôs random forest algorithm and particularly well suited for high dimensional data. The original paper describing ranger and providing benchmarking to other packages can be found here. Features include5: Classification, regression, probability estimation and survival forests are supported. Multi-threaded capabilities for optimal speed. Excellent speed and support for high-dimensional or wide data. Not as fast for ‚Äútall &amp; skinny‚Äù data (many rows, few columns). GPL-3 licensed. h2o: The h2o R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive online resource that includes methodology and code documentation along with tutorials. Features include: Automated feature pre-processing (one-hot encode &amp; standardization). Built-in cross validation. Built-in grid search capabilities. Provides automatic early stopping for faster grid searches. Supports the following distributions: ‚Äúguassian‚Äù, ‚Äúbinomial‚Äù, ‚Äúmultinomial‚Äù, ‚Äúpoisson‚Äù, ‚Äúgamma‚Äù, ‚Äútweedie‚Äù. Uses histogram approximations of continuous variables for speedup on ‚Äúlong data‚Äù (many rows). Distributed and parallelized computation on either a single node or a multi-node cluster. Model export in plain Java code for deployment in production environments. 4.4 Implementation: Regression To illustrate various regularization concepts for a regression problem we will use the Ames, IA housing data, where our intent is to predict Sale_Price. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing). 4.4.1 ranger 4.4.1.1 Basic implementation ranger::ranger uses the formula method for specifying our model. Below we apply the default ranger model specifying to model Sale_Price as a function of all features in our data set. The key arguments to the ranger call are: formula: formula specification data: training data num.trees: number of trees in the forest mtry: randomly selected predictor variables at each split. Default is \\(\\texttt{floor}(\\sqrt{\\texttt{number of features}})\\); however, for regression problems the preferred mtry to start with is \\(\\texttt{floor}(\\frac{\\texttt{number of features}}{3}) = \\texttt{floor}(\\frac{92}{3}) = 30\\) respect.unordered.factors: specifies how to treat unordered factor variables. We recommend setting this to ‚Äúorder‚Äù for regression. See Hastie et al. (2009), chapter 9.2.46 for details. seed: because this is a random algorithm, you will set the seed to get reproducible results By default, ranger will provide the computation status and estimated remaining time; however, to reduce output in this tutorial this is turned off with verbose = FALSE. As the model results show, averaging across all 500 trees provides an OOB \\(MSE = 615848303\\) (\\(RMSE \\approx 24816\\)). # number of features features &lt;- setdiff(names(ames_train), &quot;Sale_Price&quot;) # perform basic random forest model m1_ranger &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = floor(length(features) / 3), respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) # look at results m1_ranger ## Ranger result ## ## Call: ## ranger(formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = floor(length(features)/3), respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 2054 ## Number of independent variables: 80 ## Mtry: 26 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 615848303 ## R squared (OOB): 0.9013317 # compute RMSE (RMSE = square root of MSE) sqrt(m1_ranger$prediction.error) ## [1] 24816.29 One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables. However, because these methods do not require these steps does not mean you should not assess their impact. Sometimes normalizing and standardizing the data can improve performance. In the following code we compare a basic random forest model on unprocessed data to one on processed data (normalized, standardized, and zero variance features removded). # create validation set set.seed(123) split2 &lt;- initial_split(ames_train, prop = .8, strata = &quot;Sale_Price&quot;) train_tran &lt;- training(split2) validation &lt;- testing(split2) #-------------------------Unprocessed variables-------------------------# # number of features in unprocessed data m &lt;- length(setdiff(names(train_tran), &quot;Sale_Price&quot;)) # perform basic random forest model on unprocessed data m1_ranger_unprocessed &lt;- ranger( formula = Sale_Price ~ ., data = train_tran, num.trees = 500, mtry = m, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) #--------------------------Processed variables--------------------------# # preprocess features feature_process &lt;- caret::preProcess( train_tran[, features], method = c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;zv&quot;) ) train_tran &lt;- predict(feature_process, train_tran) # preprocess response train_tran$Sale_Price &lt;- log(train_tran$Sale_Price) # number of features in processed data m &lt;- length(setdiff(names(train_tran), &quot;Sale_Price&quot;)) # perform basic random forest model on processed data m1_ranger_processed &lt;- ranger( formula = Sale_Price ~ ., data = train_tran, num.trees = 500, mtry = m, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) We can now apply each model to the validation set. For the second (preprocessed) model, we re-transform our predicted values back to the normal units and we compute the RMSE for both. Now we see that our original model on unpreprocessed data is performing just as well as, if not better than, the second model on the processed data. # apply unpreprocessed model m1_pred &lt;- predict(m1_ranger_unprocessed, validation) caret::RMSE(m1_pred$predictions, validation$Sale_Price) ## [1] 22302.02 # preprocess features valid_tran &lt;- predict(feature_process, validation) # apply preprocessed model m1_tran_pred &lt;- predict(m1_ranger_processed, valid_tran) m1_processed_pred &lt;- expm1(m1_tran_pred$predictions) caret::RMSE(m1_processed_pred, validation$Sale_Price) ## [1] 24281.47 4.4.1.2 Tuning With the ranger function we can tune various hyperparameters mentioned in the general tuning section. For example, the following model adjusts: num.trees: increase number of trees to 750 mtry: reduce number of predictor variables to randomly select at each split to 20 min.node.size: reduce minimum node size to 3 (default is 5 for regression) sample.fraction: increase training set to 70% m2_ranger &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 750, mtry = 20, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = .70 ) # RMSE sqrt(m2_ranger$prediction.error) ## [1] 25298.61 # model results m2_ranger ## Ranger result ## ## Call: ## ranger(formula = Sale_Price ~ ., data = ames_train, num.trees = 750, mtry = 20, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = 0.7) ## ## Type: Regression ## Number of trees: 750 ## Sample size: 2054 ## Number of independent variables: 80 ## Mtry: 20 ## Target node size: 3 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 640019634 ## R squared (OOB): 0.8974591 We can continue to adjust these settings individually to identify the optimal combination; however, this becomes tedious when you want to explore a larger grid search. To perform a larger grid search across several hyperparameters we‚Äôll need to create a grid and loop through each hyperparameter combination and evaluate the model. First we want to construct our grid of hyperparameters. We‚Äôre going to search across 80 different models with varying number of trees, mtry, minimum node size, and sample size. # hyperparameter grid search hyper_grid &lt;- expand.grid( num.trees = seq(250, 500, 750), mtry = seq(20, 40, by = 5), node_size = seq(1, 10, by = 3), sample_size = c(.55, .632, .70, .80), OOB_RMSE = 0 ) # total number of combinations nrow(hyper_grid) ## [1] 80 # hyperparameter grid head(hyper_grid) ## num.trees mtry node_size sample_size OOB_RMSE ## 1 250 20 1 0.55 0 ## 2 250 25 1 0.55 0 ## 3 250 30 1 0.55 0 ## 4 250 35 1 0.55 0 ## 5 250 40 1 0.55 0 ## 6 250 20 4 0.55 0 We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. This full grid search ran for about 2.5 minutes before completing. Larger grid searches like these can become time consuming as your data set increases in dimensions. The h2o package provides alternative approaches to search through larger grid spaces. Our OOB RMSE ranges between ~25021-26089. Our top 10 performing models all have RMSE values in the low 25000 range and the results show that we can use a smaller number of trees than the default and models with slighly larger sample size appear to perform best. At first glance, no definitive evidence suggests that altering mtry or node_size have a sizable impact. for(i in 1:nrow(hyper_grid)) { # train model model &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, respect.unordered.factors = &#39;order&#39;, seed = 123, verbose = FALSE, mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$node_size[i], sample.fraction = hyper_grid$sample_size[i] ) # add OOB error to grid hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error) } hyper_grid %&gt;% dplyr::arrange(OOB_RMSE) %&gt;% head(10) ## num.trees mtry node_size sample_size OOB_RMSE ## 1 250 35 1 0.800 25020.85 ## 2 250 35 4 0.800 25044.19 ## 3 250 25 4 0.700 25093.52 ## 4 250 25 1 0.700 25117.58 ## 5 250 20 4 0.800 25122.44 ## 6 250 40 4 0.800 25133.63 ## 7 250 40 1 0.800 25134.92 ## 8 250 40 7 0.800 25140.17 ## 9 250 30 1 0.800 25152.47 ## 10 250 40 4 0.632 25159.07 The above grid search helps to focus where we can further refine our model tuning. As a next step, we would perform additional grid searches that focus in on a refined grid space for sample size and also try a few additional settings of mtry and min.node.size to rule out their effects on performance. However, for brevity we will leave this as an exercise for the reader. 4.4.1.3 Visual interpretation Whereas regularized regression assumes a monotonic linear relationship between features and the response, random forests make no such assumption. Moreover, random forests do not have coefficients to base these relationships on. Consequently, with random forests we can understand the relationship between the features and the response using variable importance plots and partial dependence plots. Additional model interpretability approaches will be discussed in the Model Interpretability chapter. 4.4.1.3.1 Variable importance Whereas regularized models used the standardized coefficients to signal importance, random forests have, historically, applied two different approaches to measure variable importance. Impurity: At each split in each tree, compute the improvement in the split-criterion (MSE for regression). Then average the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important. Permutation: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly ‚Äúshaking up‚Äù of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important. To compute these variable importance measures with ranger, you must include the importance argument. Once you‚Äôve identified the optimal parameter values from the grid search, you will want to re-run your model with these hyperparameter values. # re-run model with impurity-based variable importance m3_ranger_impurity &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 250, mtry = 35, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;impurity&#39; ) # re-run model with permutation-based variable importance m3_ranger_permutation &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 250, mtry = 35, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;permutation&#39; ) For both options, you can directly access the variable importance values with model_name$variable.importance. However, here we will plot the variable importance using the vip package. Typically, you will not see the same variable importance order between the two options; however, you will often see similar variables at the top of the plots. Consquently, in this example, we can comfortably state that there appears to be enough evidence to suggest that three variables stand out as most influential: Overall_Qual Gr_Liv_Area Neighborhood Looking at the next ~10 variables in both plots, you will also see some commonality in influential variables (i.e. Garage_Cars, Bsmt_Qual, Year_Built). p1 &lt;- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 4.2: Top 25 most important variables based on impurity (left) and permutation (right). 4.4.1.3.2 Partial dependence plots After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. Unlike linear approaches, random forests do not assume a linear relationship. Consequently, we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves. PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. For example, consider the Gr_Liv_Area variable. In the h20 regularized regression section (3.4.2.3), we saw that the linear model assumed a continously increasing relationship between Gr_Liv_Area and Sale_Price. However, the PDP plot below displays a non-linear relationship where Sale_Price appears to not be influenced by Gr_Liv_Area values below 750 sqft or above 3500 sqft. # partial dependence of Sale_Price on Gr_Liv_Area m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50) %&gt;% autoplot(rug = TRUE, train = ames_train) Figure 4.3: The mean predicted sale price as the above ground living area increases. Additionally, if we assess the relationship between the Overall_Qual predictor and Sale_Price, we see a continual increase as the overall quality increases. This is more intutive than the results we saw in the regularized regression section (3.4.2.3). This may be an indication that the coefficients were biased in the regularized regression models. # partial dependence of Sale_Price on Overall_Qual m3_ranger_impurity %&gt;% partial(pred.var = &quot;Overall_Qual&quot;, train = as.data.frame(ames_train)) %&gt;% autoplot() Figure 4.4: The mean predicted sale price for each level of the overall quality variable. Individual conditional expectation (ICE) curves (Goldstein et al. 2015) are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently ‚Äústacked‚Äù on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. The plots below show that marginal changes in Gr_Liv_Area have a fairly homogenous effect on our response variable. As Gr_Liv_Area increases, the vast majority of observations show a similar increasing effect on the predicted Sale_Price value. The primary differences is in the magnitude of the increasing effect. However, in the centered ICE plot you see evidence of a few observations that display a different pattern. These observations would be worth looking at more closely. # ice curves of Sale_Price on Gr_Liv_Area ice1 &lt;- m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.2) + ggtitle(&quot;Non-centered ICE plot&quot;) ice2 &lt;- m3_ranger_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.2, center = TRUE) + ggtitle(&quot;Centered ICE plot&quot;) gridExtra::grid.arrange(ice1, ice2, nrow = 1) Figure 4.5: Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations. Both PDPs and ICE curves should be assessed for the most influential variables as they help to explain the underlying patterns in the data that the random forest model is picking up. Check out the Model Interpretation chapter to learn more about visualizing your machine learning models. 4.4.1.4 Predicting Once you‚Äôve found your optimal model, predicting new observations with the ranger model follows the same procedure as most R models. We can apply the predict function and supply it the optimal model and the new data set we‚Äôd like to predict on. The result is a list object that includes several attributes about the model used to predict (i.e. number of trees &amp; predictor variables, sample size, tree type). The predicted values we are most concerned with are contained in the predict_object$predictions list item. # predict on test set predict_ranger &lt;- predict(m3_ranger_impurity, ames_test) # predict object str(predict_ranger) ## List of 5 ## $ predictions : num [1:876] 130020 157044 224076 251814 376064 ... ## $ num.trees : num 250 ## $ num.independent.variables: num 80 ## $ num.samples : int 876 ## $ treetype : chr &quot;Regression&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ranger.prediction&quot; # predicted values head(predict_ranger$predictions) ## [1] 130020.1 157044.1 224076.1 251813.8 376064.3 365273.1 We can use these predicted values to assess the final generalization error, which is slightly lower than our models OOB sample RMSE: # final model OOB RMSE sqrt(m3_ranger_impurity$prediction.error) ## [1] 25197.67 # generalization error caret::RMSE(predict_ranger$predictions, ames_test$Sale_Price) ## [1] 25148.62 4.4.2 h20 To perform a random forest model with h2o, we first need to initiate our h2o session. h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpMBcJyZ/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpMBcJyZ/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 247 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_ply740 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we need to convert our training and test data to h2o objects. # convert training data to h2o object train_h2o &lt;- as.h2o(ames_train) # convert test data to h2o object test_h2o &lt;- as.h2o(ames_test) # set the response column to Sale_Price response &lt;- &quot;Sale_Price&quot; # set the predictor names predictors &lt;- setdiff(colnames(ames_train), response) 4.4.2.1 Basic implementation To perform a random forest model with h2o we use h2o::h2o.randomForest. Keep in mind that h2o uses the name method for specifying our model. Below we apply the default h2o.randomForest model specifying to model Sale_Price as a function of all features in our data set. h2o.randomForest has many arguments that can be adjusted; however, often the default settings perform very well. To start with, a few key arguments in h2o.randomForest to understand include: x: names of the predictor variables y: name of the response variable training_frame: training data ntrees: number of trees in the forest (default is 50) mtries: randomly selected predictor variables at each split. Default is \\(\\texttt{floor}(\\frac{\\texttt{number of features}}{3}) = \\texttt{floor}(\\frac{92}{3}) = 30\\) for regression. categorical_encoding: Decides the encoding scheme for categorical variables. Typically choose one of ‚ÄúEnum‚Äù or ‚ÄúSortByResponse‚Äù (categorical_encoding = 'SortByResponse' performs similar procedure as respect.unordered.factors = 'order'). For these data we do not see any difference in performance between the two. seed: because this is a random algorithm, you will set the seed to get reproducible results h2o can provide the computation status; however, this feature is turned off by default but can be turned on with verbose = FALSE. As the model results show, averaging across all 250 trees provides an OOB \\(RMSE \\approx 24541\\)). These are pretty similar to what we found with the default ranger model. # perform basic random forest model m1_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, ntrees = 250, seed = 123 ) # look at results ## m1_h2o ## Model Details: ## ============== ## ## H2ORegressionModel: drf ## Model ID: DRF_model_R_1532981766487_1 ## Model Summary: ## ## ## H2ORegressionMetrics: drf ## ** Reported on training data. ** ## ** Metrics reported on Out-Of-Bag training samples ** ## ## MSE: 602273377 ## RMSE: 24541.26 ## MAE: 15060.85 ## RMSLE: 0.1406867 ## Mean Residual Deviance : 602273377 One of the benefits of h2o is it allows us to include stopping_ arguments, which will stop the modeling automatically once the RMSE metric on the OOB samples stops improving by a certain value (say 1%) for a specified number of consecutive trees. This helps us to identify the number of trees required to stabilize our error metric. Below we see that 49 trees are sufficient. # perform basic random forest model m2_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = train_h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, # stopping mechanism stopping_rounds = 10, # number of rounds stopping_tolerance = 0.005 # looking for 0.5% improvement ) # look at results m2_h2o ## Model Details: ## ============== ## ## H2ORegressionModel: drf ## Model ID: DRF_model_R_1532981766487_2 ## Model Summary: ## number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves ## 1 49 49 758326 20 20 20.00000 1175 1273 1226.34690 ## ## ## H2ORegressionMetrics: drf ## ** Reported on training data. ** ## ** Metrics reported on Out-Of-Bag training samples ** ## ## MSE: 670676969 ## RMSE: 25897.43 ## MAE: 15741.89 ## RMSLE: 0.1434247 ## Mean Residual Deviance : 670676969 4.4.2.2 Tuning h2o.randomForest provides many tuning options. The more common tuning options can be categorized into three purposes: Controlling how big your random forest will be: ntrees: how many trees in the forest max_depth: maximum depth to which each tree will be built (default is 20) Controlling the random components of the model: mtries: number of predictor variables to randomly select at each split sample_rate: Row sample rate per tree (default is 63.2%) Controlling how the splitting is done min_rows: minimum number of observations for a leaf in order to split (default is 1) There are additional tuning parameters in each of the above categories but their defaults are typically sufficient. You can read about the options here. We can tune these hyperparameters individually; however, a major benefit of h2o is it provides two different approaches for hyperparameter grid searches: Full cartesian grid search: examine every combination of hyperparameter settings that we specify, Random grid search: jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). 4.4.2.2.1 Full cartesian grid search First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in hyper_grid.h2o. Here, we search across 320 hyperparameter combinations. You can include ntrees as a hyperparameter; however, its more efficient to set ntrees to a high value and then use early stopping to stop each model once improvement is no longer obtained. This comprehensive grid search took 56 minutes. The results show a minimum RMSE of $23,792 (slightly less than our optimal ranger model), when max_depth = 25, min_rows = 1, mtries = 25, and sample_rate = 80%. Looking at the top 5 models it appears that the primary driving parameters for minimizing MSE are min_rows (smaller is better), mtries (smaller is better), and sample_rate (larger is better). # hyperparameter grid hyper_grid.h2o &lt;- list( mtries = seq(20, 40, by = 5), max_depth = seq(15, 30, by = 5), min_rows = seq(1, 10, by = 3), sample_rate = c(.55, .632, .70, .80) ) # build grid search grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_full_grid&quot;, x = predictors, y = response, training_frame = train_h2o, hyper_params = hyper_grid.h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, stopping_rounds = 10, stopping_tolerance = 0.005, search_criteria = list(strategy = &quot;Cartesian&quot;) ) # collect the results and sort by our model performance metric of choice full_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_full_grid&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) print(full_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_full_grid ## Used hyper parameters: ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 291 ## Number of failed models: 29 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## max_depth min_rows mtries sample_rate model_ids mse ## 1 25 1.0 25 0.8 rf_full_grid_model_258 5.660696269742714E8 ## 2 30 1.0 25 0.8 rf_full_grid_model_259 5.66075155855145E8 ## 3 20 1.0 25 0.8 rf_full_grid_model_257 5.66146259929908E8 ## 4 30 1.0 20 0.8 rf_full_grid_model_243 5.665494562778755E8 ## 5 25 1.0 20 0.8 rf_full_grid_model_242 5.665555943619757E8 ## ## --- ## max_depth min_rows mtries sample_rate model_ids mse ## 286 25 10.0 35 0.55 rf_full_grid_model_62 8.195309220382509E8 ## 287 20 10.0 35 0.55 rf_full_grid_model_61 8.195309220382509E8 ## 288 30 10.0 30 0.632 rf_full_grid_model_127 8.266536241339123E8 ## 289 25 10.0 30 0.632 rf_full_grid_model_126 8.266536241339123E8 ## 290 20 10.0 30 0.632 rf_full_grid_model_125 8.266536241339123E8 ## 291 15 10.0 30 0.632 rf_full_grid_model_124 8.266536241339123E8 4.4.2.2.2 Random discrete grid search Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, h2o provides an additional grid search path called ‚ÄúRandomDiscrete‚Äù, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model. This comprehensive grid search took 30 minutes. For example, the following code searches the same grid search performed above. We create a random grid search that will stop if none of the last 10 models have managed to have a 0.1% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 1800 seconds (30 minutes). Our grid search assessed 191 models before stopping due to time. The best model (max_depth = 25, min_rows = 1, mtries = 40, and sample_rate = 0.8) achived an RMSE \\(\\approx \\$23,751\\). So although our random search on assessed about half the number of models as the full grid search, the more efficient random search found a near-optimal model relatively speaking. # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;mse&quot;, stopping_tolerance = 0.001, stopping_rounds = 10, max_runtime_secs = 60*30 ) # build grid search random_grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_random_grid&quot;, x = predictors, y = response, training_frame = train_h2o, hyper_params = hyper_grid.h2o, ntrees = 500, seed = 123, stopping_metric = &quot;RMSE&quot;, stopping_rounds = 10, stopping_tolerance = 0.005, search_criteria = search_criteria ) # collect the results and sort by our model performance metric of choice random_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_random_grid&quot;, sort_by = &quot;mse&quot;, decreasing = FALSE ) print(random_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_random_grid ## Used hyper parameters: ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 191 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing mse ## max_depth min_rows mtries sample_rate model_ids mse ## 1 25 1.0 40 0.8 rf_random_grid_model_131 5.624310085353142E8 ## 2 15 1.0 40 0.8 rf_random_grid_model_180 5.63276905670922E8 ## 3 25 1.0 25 0.8 rf_random_grid_model_174 5.660696269742714E8 ## 4 20 1.0 25 0.8 rf_random_grid_model_17 5.66146259929908E8 ## 5 20 1.0 20 0.8 rf_random_grid_model_144 5.66631854910229E8 ## ## --- ## max_depth min_rows mtries sample_rate model_ids mse ## 186 20 10.0 40 0.55 rf_random_grid_model_52 8.163555317753098E8 ## 187 30 10.0 30 0.8 rf_random_grid_model_50 8.182002205674793E8 ## 188 20 10.0 35 0.55 rf_random_grid_model_23 8.195309220382509E8 ## 189 30 10.0 35 0.55 rf_random_grid_model_97 8.195309220382509E8 ## 190 20 10.0 30 0.632 rf_random_grid_model_44 8.266536241339123E8 ## 191 30 7.0 30 0.8 rf_random_grid_model_190 1.3296523023487797E9 Once we‚Äôve identifed the best model we can extract it with: # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- random_grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) 4.4.2.3 Visualizing results 4.4.2.3.1 Variable importance Once you‚Äôve identified and selected the optimally tuned model, you can visualize variable importance with h2o.varimp_plot. h2o.varimp_plot computes variable importance ‚Äúby calculating the relative influence of each variable: whether that variable was selected during splitting in the tree building process and how much the squared error (over all trees) improved as a result.‚Äù7 This is equivalent to the impurity approach used by ranger. The most important variables are relatively similar to those found with the ranger model. h2o.varimp_plot(best_model, num_of_features = 25) Figure 4.6: Variable importance plot provided by the h2o package. If you prefer the plotting provided by the vip package, you can also use vip::vip on any h2o model as well. vip(best_model, num_features = 25, bar = FALSE) Figure 4.7: Variable importance plot provided by the vip package. 4.4.2.3.2 Partial dependence plots As with ranger, we can also assess PDP plots. h2o provides the h2o.partialPlot function to plot PDPs. Although it does not allow you to plot individual ICE curves, it does plot the standard error of the mean response across all observations along with automatically showing you the values of the predictor variable (by default it selects 20 values but can be adjusted with nbins), mean response, and standard error of the response. The partial dependence plot for Gr_Liv_Area follows a similar non-linear trend as we saw with the ranger model. h2o.partialPlot(best_model, data = train_h2o, cols = &quot;Gr_Liv_Area&quot;) Figure 4.8: h2o‚Äôs partial dependence plot of the Gr_Liv_Area predictor variable based on the optimal h2o model. ## PartialDependence: Partial Dependence Plot of model rf_random_grid_model_131 on column &#39;Gr_Liv_Area&#39; ## Gr_Liv_Area mean_response stddev_response ## 1 334.000000 161930.609931 63443.765820 ## 2 613.368421 162062.529779 63315.589233 ## 3 892.736842 162452.265339 63060.091227 ## 4 1172.105263 167315.601270 61620.417433 ## 5 1451.473684 176012.418557 59599.978531 ## 6 1730.842105 185785.468561 64362.546404 ## 7 2010.210526 195264.075938 71031.464662 ## 8 2289.578947 201998.046236 73740.380295 ## 9 2568.947368 206241.151641 74596.263303 ## 10 2848.315789 209652.185877 75251.557070 ## 11 3127.684211 210836.830736 74650.327138 ## 12 3407.052632 211658.584116 75789.811261 ## 13 3686.421053 211833.145557 76111.081331 ## 14 3965.789474 211755.931914 75912.587977 ## 15 4245.157895 211461.025415 75172.912270 ## 16 4524.526316 211422.033670 75065.495828 ## 17 4803.894737 211373.627191 74921.530746 ## 18 5083.263158 211373.486977 74921.244177 ## 19 5362.631579 211369.474903 74913.694244 ## 20 5642.000000 211369.474903 74913.694244 If you prefer getting actual ICE curves, we can use the pdp package. However, since pdp does not have an explicit method for h2o objects we need to create a prediction function and use the pred.fun argument: # build custom prediction function pfun &lt;- function(object, newdata) { as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]] } # compute ICE curves prod.ice &lt;- partial( best_model, pred.var = &quot;Gr_Liv_Area&quot;, train = ames_train, pred.fun = pfun, grid.resolution = 20 ) p1 &lt;- autoplot(prod.ice, alpha = 0.2) + ggtitle(&quot;Non-centered ICE curves&quot;) p2 &lt;- autoplot(prod.ice, alpha = 0.2, center = TRUE) + ggtitle(&quot;Centered ICE curves&quot;) gridExtra::grid.arrange(p1, p2, ncol = 2) Figure 4.9: pdp‚Äôs ICE curves of the Gr_Liv_Area predictor variable based on the optimal h2o model. 4.4.2.4 Predicting Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways. We can also quickly assess the model‚Äôs performance on our test set with h2o.performance. We see a similar generalizable error as we saw with the ranger model. # predict new values with base R predict() predict(best_model, test_h2o) ## predict ## 1 129214.6 ## 2 155571.0 ## 3 224196.0 ## 4 250114.6 ## 5 359731.0 ## 6 362031.2 ## ## [876 rows x 1 column] # predict new values with h2o.predict() h2o.predict(best_model, newdata = test_h2o) ## predict ## 1 129214.6 ## 2 155571.0 ## 3 224196.0 ## 4 250114.6 ## 5 359731.0 ## 6 362031.2 ## ## [876 rows x 1 column] # assess performance on test data h2o.performance(best_model, newdata = test_h2o) ## H2ORegressionMetrics: drf ## ## MSE: 661358432 ## RMSE: 25716.89 ## MAE: 15628.89 ## RMSLE: 0.1249999 ## Mean Residual Deviance : 661358432 # shut down h2o h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.5 Implementation: Binary Classification To illustrate random forests concepts for a binary classification problem we will continue with the employee attrition data. attrition &lt;- rsample::attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) %&gt;% mutate(Attrition = relevel(Attrition, ref = &quot;Yes&quot;)) # Create training and testing sets set.seed(123) split &lt;- initial_split(attrition, prop = .8, strata = &quot;Attrition&quot;) attrit_train &lt;- training(split) attrit_test &lt;- testing(split) Tree-based algorithms typically perform very well without preprocessing the data (i.e. one-hot encoding, normalizing, standardizing). 4.5.1 ranger 4.5.1.1 Basic implementation We apply ranger::ranger just as we did in the regression setting. However, note that the default mtry is \\(\\texttt{floor}(\\sqrt{\\texttt{number of features}})\\), which is a good starting point for classification problems (we changed it to \\(mtry = \\texttt{floor}(\\frac{\\texttt{number of features}}{3})\\) in the regression setting). As long as your response variable is encoded as a character or factor, ranger will automatically perform a classification random forest model. As the model results show, majority voting across all 500 trees provides an OOB error rate of 13.59%. # perform basic random forest model m1_ranger &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123 ) # look at results m1_ranger ## Ranger result ## ## Call: ## ranger(formula = Attrition ~ ., data = attrit_train, num.trees = 500, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 1177 ## Number of independent variables: 30 ## Mtry: 5 ## Target node size: 1 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 13.59 % # look at confusion matrix m1_ranger$confusion.matrix ## predicted ## true Yes No ## Yes 36 154 ## No 6 981 The default ranger classification model does not provide probability estimates. If you want to predict the probabilities then use probability = TRUE. When using this option, the OOB prediction error changes from misclassification rate to MSE. One of the benefits of tree-based methods is they do not require preprocessing steps such as normalization and standardization of the response and/or predictor variables. However, because these methods do not require these steps does not mean you should not assess their impact. Sometimes normalizing and standardizing the data can improve performance. In the following code we compare a basic random forest probability model with unprocessed features to one with processed features (normalized, standardized, and zero variance features removed). # create validation set set.seed(123) split2 &lt;- initial_split(attrit_train, prop = .8, strata = &quot;Attrition&quot;) train_tran &lt;- training(split2) validation &lt;- testing(split2) #-------------------------Unprocessed variables-------------------------# # perform basic random forest model on unprocessed data m1_ranger_unprocessed &lt;- ranger( formula = Attrition ~ ., data = train_tran, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, probability = TRUE ) #--------------------------Processed variables--------------------------# features &lt;- setdiff(names(attrit_train), &quot;Attrition&quot;) # preprocess features feature_process &lt;- caret::preProcess( train_tran[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;zv&quot;) ) train_tran &lt;- predict(feature_process, train_tran) # perform basic random forest model on processed data m1_ranger_processed &lt;- ranger( formula = Attrition ~ ., data = train_tran, num.trees = 500, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, probability = TRUE ) We can now apply each model to the validation set and we see that the feature processing has no impact on our area under the curve. # apply unpreprocessed model m1_pred &lt;- predict(m1_ranger_unprocessed, validation) roc &lt;- pROC::roc(validation$Attrition, m1_pred$predictions[, 1]) pROC::auc(roc) ## Area under the curve: 0.8586 # preprocess features valid_tran &lt;- predict(feature_process, validation) # apply preprocessed model m1_tran_pred &lt;- predict(m1_ranger_processed, valid_tran) roc &lt;- pROC::roc(validation$Attrition, m1_tran_pred$predictions[, 1]) pROC::auc(roc) ## Area under the curve: 0.8591 4.5.1.2 Tuning With the ranger function we can tune various hyperparameters mentioned in the general tuning section. For example, the following model adjusts: num.trees: increase number of trees to 750 mtry: increase the number of predictor variables to randomly select at each split to 20 min.node.size: increase minimum node size to 3 (default is 1 for classification) sample.fraction: increase training set to 70% m2_ranger &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 750, mtry = 20, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = .70 ) # misclassification rate m2_ranger$prediction.error ## [1] 0.1393373 # model results m2_ranger ## Ranger result ## ## Call: ## ranger(formula = Attrition ~ ., data = attrit_train, num.trees = 750, mtry = 20, respect.unordered.factors = &quot;order&quot;, verbose = FALSE, seed = 123, min.node.size = 3, sample.fraction = 0.7) ## ## Type: Classification ## Number of trees: 750 ## Sample size: 1177 ## Number of independent variables: 30 ## Mtry: 20 ## Target node size: 3 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 13.93 % We can continue to adjust these settings individually to identify the optimal combination; however, this become tedious when you want to explore a larger grid search. Similar to the regression setting, to perform a larger grid search across several hyperparameters we need to create a grid and loop through each hyperparameter combination and evaluate the model. First we want to construct our grid of hyperparameters. We‚Äôre going to search across 200 different models with varying number of trees, mtry, minimum node size, and sample size. I also vary the split rule, which determines when and how to split into branches. # hyperparameter grid search hyper_grid &lt;- expand.grid( num.trees = seq(250, 500, 750), mtry = seq(5, 25, by = 5), node_size = seq(1, 10, by = 3), sample_size = c(.55, .632, .70, .80, 1), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), OOB_error = 0 ) # total number of combinations nrow(hyper_grid) ## [1] 200 # hyperparameter grid head(hyper_grid) ## num.trees mtry node_size sample_size splitrule OOB_error ## 1 250 5 1 0.55 gini 0 ## 2 250 10 1 0.55 gini 0 ## 3 250 15 1 0.55 gini 0 ## 4 250 20 1 0.55 gini 0 ## 5 250 25 1 0.55 gini 0 ## 6 250 5 4 0.55 gini 0 We can now loop through each hyperparameter combination. Note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. This full grid search took about 90 seconds to compute. Our OOB classification error ranges between ~0.1308-0.1487. Our top 10 performing models all have classification error rates in the lower 0.13 range. The results show that all the top 10 models use less trees, larger mtry than the default (\\(\\text{floor}\\big(\\sqrt{\\text{number of features}}\\big) = 5\\), and a sample size less than 100. However, no definitive patterns are observed with the other hyperparameters. for(i in 1:nrow(hyper_grid)) { # train model model &lt;- ranger( formula = Attrition ~ ., data = attrit_train, respect.unordered.factors = &#39;order&#39;, seed = 123, verbose = FALSE, mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$node_size[i], sample.fraction = hyper_grid$sample_size[i], splitrule = hyper_grid$splitrule[i] ) # add OOB error to grid hyper_grid$OOB_error[i] &lt;- model$prediction.error } hyper_grid %&gt;% dplyr::arrange(OOB_error) %&gt;% head(10) ## num.trees mtry node_size sample_size splitrule OOB_error ## 1 250 25 1 0.800 gini 0.1308411 ## 2 250 20 7 0.632 gini 0.1325404 ## 3 250 25 10 0.800 extratrees 0.1325404 ## 4 250 25 4 0.700 gini 0.1333900 ## 5 250 25 7 0.700 gini 0.1333900 ## 6 250 20 7 0.550 extratrees 0.1333900 ## 7 250 25 10 0.700 extratrees 0.1333900 ## 8 250 25 4 0.800 extratrees 0.1333900 ## 9 250 25 1 0.700 gini 0.1342396 ## 10 250 10 7 1.000 gini 0.1342396 The above grid search helps to focus where we can further refine our model tuning. As a next step, we would perform additional grid searches; however, for brevity we will leave this as an exercise for the reader. 4.5.1.3 Visualizing results 4.5.1.3.1 Variable importance As in the regression setting, once we‚Äôve found our optimal hyperparameter settings we can re-run our model and set the importance argument to ‚Äúimpurity‚Äù and/or ‚Äúpermutation‚Äù. The following applies both settings so that we can compare and contrast the influential variables each method identifies. m3_ranger_impurity &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;impurity&#39; ) m3_ranger_permutation &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, importance = &#39;permutation&#39; ) Plotting the top 25 influential variables using both variable importance methods results in a common theme among the top 3 variables - MonthlyIncome, Age, and OverTime appear to have strong influence on our results. We also saw OverTime as an influential variable using regularized regression. Looking at the next dozen important variables, we see similar results across variable importance approaches but just in different order (i.e. TotalWorkingYears, JobRole, NumCompaniesWorked). Some of these were influential variables in the regularized regression models and some were not; suggesting our random forest model is picking up different patterns and logic in our data. p1 &lt;- vip(m3_ranger_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(m3_ranger_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 4.10: Top 25 most important variables based on impurity (left) and permutation (right). 4.5.1.3.2 Partial dependence plots After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. This is important considering random forests allow us to pick up non-linear, non-monotonic relationships. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves. However, to generate PDPs and ICE curves we need to run a probability model (probability = TRUE) so that we can extract the class probabilities. To produce a PDP with binary classification problems, we need to create a custom prediction function that will return a vector of the mean predicted probability for the response class of interest (in this example we want the probabilities for Attrition = ‚ÄúYes‚Äù). We supply this custom prediction function Our PDPs illustrate a strong increase in the probability of attrition for employees that work overtime. Also, note that non-linear relationship between the probability of attrition and monthly income and age. The MonthlyIncome plot shows an increase in probability as monthly income reaches $10,000 but then flatlines until employees make about $20,000 per month. Similiarly with age, as employees get older they tend to become more stable; however, this changes after the age of 45 where an increase of age tends to increase the probablity of attrition (recall in Section 3.5.2.3 that we saw how the regularized models assumed a constantly decreasing relationships between Age and the probability of attrition). # probability model m3_ranger_prob &lt;- ranger( formula = Attrition ~ ., data = attrit_train, num.trees = 250, mtry = 25, respect.unordered.factors = &#39;order&#39;, verbose = FALSE, seed = 123, min.node.size = 1, sample.fraction = .80, probability = TRUE, importance = &#39;impurity&#39; ) # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, newdata) avg &lt;- mean(pred$predictions[, 1]) return(avg) } # partial dependence of OverTime p1 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;OverTime&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) # partial dependence of MonthlyIncome p2 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;MonthlyIncome&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) # partial dependence of Age p3 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;Age&quot;, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.11: Partial dependence plots of our top 3 influential variables. Note the non-linear, non-monotonic relationship our random forest model is picking up for MonthlyIncome and Age. We can extract more insights with centered ICE curves. Although the PDP illustrates an increase in the average probability of attrition for employees who work overtime, the ICE curves illustrate that this is not the case for all employees. A fair amount of the observations actually experience a decrease in probability when they work overtime. This likely suggests an intereaction effect with other variables (we will discuss how to tease out interactions in the Model Interpretability chapter). To produce ICE curves with binary classification problems, we need to create a custom prediction function that will return a vector of the predicted probabilities for the response class of interest. # custom prediction function custom_pred &lt;- function(object, newdata) { pred &lt;- predict(object, newdata) avg &lt;- pred$predictions[, 1] return(avg) } # ICE curves for top 3 influential variables p1 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;OverTime&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) p2 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;MonthlyIncome&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) p3 &lt;- m3_ranger_prob %&gt;% partial(pred.var = &quot;Age&quot;, ice = TRUE, center = TRUE, pred.fun = custom_pred, train = attrit_train) %&gt;% autoplot(rug = TRUE, train = attrit_train, alpha = 0.2) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.12: Centered ICE curves for our top 3 influential variables. 4.5.1.3.3 ROC curve As in the regularize regression chapter, we can visualize the ROC curve with the ROCR and pROC packages. Both packages compare the predicted probability output to the actual observed class so we need to use our probability ranger model m3_ranger_prob. The predicted probabilities for our model are accessible at ranger_model$predictions and we want to index for the class of interest. library(ROCR) library(pROC) # plot structure par(mfrow = c(1, 2)) # ROCR plot prediction(m3_ranger_prob$predictions[, 1], attrit_train$Attrition) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) %&gt;% plot(main = &quot;ROCR ROC curve&quot;) #pROC plot roc(attrit_train$Attrition, m3_ranger_prob$predictions[, 1]) %&gt;% plot(main = &quot;pROC ROC curve&quot;, legacy.axes = TRUE) Figure 4.13: ROC curve for our ranger random forest model based on the training data. 4.5.1.4 Predicting Once you have identified your preferred model, you can simply use predict to predict the same model on a new data set. If you use a probability model, the predicted values will be probabilities for each class. If you use a non-probability model, the predicted values will be the class. # predict a probability model pred_probs &lt;- predict(m3_ranger_prob, attrit_test) head(pred_probs$predictions) ## Yes No ## [1,] 0.124 0.876 ## [2,] 0.472 0.528 ## [3,] 0.056 0.944 ## [4,] 0.064 0.936 ## [5,] 0.296 0.704 ## [6,] 0.124 0.876 # predict a non-probability model pred_class &lt;- predict(m3_ranger_impurity, attrit_test) head(pred_class$predictions) ## [1] No No No No No No ## Levels: Yes No Lastly, to assess various performance metrics on our test data we use caret::confusionMatrix, which provides the majority of the performance measures we are typically concerned with in classification models. If you compare the results to the regularized regression model you will notice that our random forest model does not provide additional predictive performance. You need to supply caret::confusionMatrix with the predicted class. Consequently, if you use the probability model which predicts probabilities then you will need perform an extract step that creates a predicted class based on the probabilities (i.e. ifelse(probability &gt;= .5, ‚ÄúYes‚Äù, ‚ÄúNo‚Äù)). caret::confusionMatrix(factor(pred_class$predictions), attrit_test$Attrition, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 13 5 ## No 34 241 ## ## Accuracy : 0.8669 ## 95% CI : (0.8226, 0.9036) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.1145 ## ## Kappa : 0.3415 ## Mcnemar&#39;s Test P-Value : 7.34e-06 ## ## Sensitivity : 0.27660 ## Specificity : 0.97967 ## Pos Pred Value : 0.72222 ## Neg Pred Value : 0.87636 ## Prevalence : 0.16041 ## Detection Rate : 0.04437 ## Detection Prevalence : 0.06143 ## Balanced Accuracy : 0.62814 ## ## &#39;Positive&#39; Class : Yes ## 4.5.2 h20 To perform a binary classification random forest with h2o, we first need to initiate our h2o session. # launch h2o h2o::h2o.no_progress() h2o.init(max_mem_size = &quot;5g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 500 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.11 ## H2O cluster version age: 2 months and 8 days ## H2O cluster name: H2O_started_from_R_bradboehmke_ply740 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 4.44 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.5.1 (2018-07-02) Next, we need to convert our training and test data to h2o objects. # convert training data to h2o object attrit_train_h2o &lt;- as.h2o(attrit_train) # convert test data to h2o object attrit_test_h2o &lt;- as.h2o(attrit_test) # set the response column to Attrition response &lt;- &quot;Attrition&quot; # set the predictor names predictors &lt;- setdiff(colnames(attrit_train), &quot;Attrition&quot;) 4.5.2.1 Basic implementation Similar to our regression problem, we use h2o::h2o.randomForest to perform a random forest model with h2o. Most of the default parameter settings in h2o.randomForest do not change between a regression and classification problem. However, mtries (how many predictor variables are randomly selected at each split) defaults to \\(\\texttt{floor}(\\sqrt{p})\\). As long as your response variable is encoded as a character or factor, h2o will apply a binomial or multinomial classification model. Alternatively, you can specify the response distribution with the distribution argument. The following performs a default h2o.randomForest model with 250 trees and a 10 fold cross validation. As the model results show, averaging across all 250 trees provides an OOB \\(AUC = 0.8\\). # perform basic random forest model m1_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = attrit_train_h2o, ntrees = 250, seed = 123, nfolds = 10, keep_cross_validation_predictions = TRUE ) # look at results h2o.performance(m1_h2o, xval = TRUE) ## H2OBinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.1076479 ## RMSE: 0.3280973 ## LogLoss: 0.3600018 ## Mean Per-Class Error: 0.2996321 ## AUC: 0.795918 ## Gini: 0.591836 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 915 72 0.072948 =72/987 ## Yes 100 90 0.526316 =100/190 ## Totals 1015 162 0.146134 =172/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.304000 0.511364 81 ## 2 max f2 0.164000 0.607055 130 ## 3 max f0point5 0.352000 0.579365 65 ## 4 max accuracy 0.356000 0.869159 64 ## 5 max precision 0.754667 1.000000 0 ## 6 max recall 0.016000 1.000000 177 ## 7 max specificity 0.754667 1.000000 0 ## 8 max absolute_mcc 0.352000 0.438268 65 ## 9 max min_per_class_accuracy 0.180000 0.727457 124 ## 10 max mean_per_class_accuracy 0.220000 0.741097 108 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` The results above used all 250 trees but to make sure we are providing enough trees to stabilize the OOB error we can include automatic stopping. Also, when dealing with classification problems, if your response variable is significantly imbalanced, you can achieve additional predictive accuracy by over/under sampling. We can over/under sample our classes to achieved balanced class counts by incorporating the balance_classes argument. However, we do not achieve any performance improvement by balancing our attrition classes. You will, typically, achieve performance improvements by over/under sampling when you binary response variable has a 90/10 or worse class imbalance. # perform basic random forest model m2_h2o &lt;- h2o.randomForest( x = predictors, y = response, training_frame = attrit_train_h2o, ntrees = 500, seed = 123, nfolds = 10, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, stopping_metric = &quot;AUC&quot;, # stopping mechanism stopping_rounds = 10, # number of rounds stopping_tolerance = 0 # stops after trees add no improvement ) # look at results h2o.performance(m2_h2o, xval = TRUE) ## H2OBinomialMetrics: drf ## ** Reported on cross-validation data. ** ## ** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** ## ## MSE: 0.129077 ## RMSE: 0.3592728 ## LogLoss: 0.4325378 ## Mean Per-Class Error: 0.2804218 ## AUC: 0.7969045 ## Gini: 0.593809 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 875 112 0.113475 =112/987 ## Yes 85 105 0.447368 =85/190 ## Totals 960 217 0.167375 =197/1177 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.086292 0.515971 143 ## 2 max f2 0.044498 0.602837 239 ## 3 max f0point5 0.135658 0.574205 78 ## 4 max accuracy 0.135658 0.869159 78 ## 5 max precision 0.570324 1.000000 0 ## 6 max recall 0.008554 1.000000 383 ## 7 max specificity 0.570324 1.000000 0 ## 8 max absolute_mcc 0.135658 0.424441 78 ## 9 max min_per_class_accuracy 0.052971 0.721378 216 ## 10 max mean_per_class_accuracy 0.056358 0.730742 207 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` 4.5.2.2 Tuning As discussed in the regression section of this chapter, h2o.randomForest provides several tunable hyperparameters, for which we can perform a full (aka full cartesian) or stochastic (aka random discrete) grid search across. 4.5.2.2.1 Full cartesian grid search First, we can try a comprehensive (full cartesian) grid search, which means we will examine every combination of hyperparameter settings that we specify in hyper_grid.h2o. Here, we search across 360 models. To speed up the grid search I dropped it down to 5-fold cross validation; however, this grid search still took 30 minutes. As an alternative, you could create a single validation frame (see validation_frame in ?h2o.grid) to score against rather than perform k-fold cross validation. The results show a maximum AUC of 1 but don‚Äôt get too excited about this. When you over/under sample with balance_classes = TRUE, we are essentially bootstrapping extra samples of the observations with Attrition = Yes. This means our up-sampled observations will have many of the same values across the features. This makes it easier to over exaggerate the predictive performance during our training. A few characteristics we notice from our results suggest that predictive accuracy is maximized when balance_classes = TRUE, max_depth is larger, min_rows is smaller, and sample_rate \\(&lt; 1\\). # hyperparameter grid hyper_grid.h2o &lt;- list( mtries = c(2, 5, 10, 15), max_depth = seq(10, 30, by = 5), min_rows = c(1, 3, 5), sample_rate = c(.632, .8, .95), balance_classes = c(TRUE, FALSE) ) # build grid search grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_full_grid&quot;, x = predictors, y = response, training_frame = attrit_train_h2o, hyper_params = hyper_grid.h2o, search_criteria = list(strategy = &quot;Cartesian&quot;), ntrees = 500, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;AUC&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) # collect the results and sort by our model performance metric of choice full_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_full_grid&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE ) print(full_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_full_grid ## Used hyper parameters: ## - balance_classes ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 720 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by decreasing auc ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 1 true 25 1.0 2 0.632 rf_full_grid_model_6 1.0 ## 2 true 30 1.0 10 0.95 rf_full_grid_model_308 1.0 ## 3 true 30 1.0 2 0.632 rf_full_grid_model_8 1.0 ## 4 true 30 1.0 5 0.8 rf_full_grid_model_158 1.0 ## 5 true 15 1.0 10 0.8 rf_full_grid_model_182 1.0 ## ## --- ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 715 false 10 1.0 10 0.95 rf_full_grid_model_301 0.6200193262072915 ## 716 false 25 3.0 5 0.95 rf_full_grid_model_287 0.6037350054525626 ## 717 false 10 1.0 15 0.95 rf_full_grid_model_331 0.6013023735068234 ## 718 false 10 1.0 2 0.95 rf_full_grid_model_241 0.5981035997865863 ## 719 false 25 1.0 10 0.95 rf_full_grid_model_307 0.5896310022558814 ## 720 false 15 1.0 2 0.95 rf_full_grid_model_243 0.5875399042298484 4.5.2.2.2 Random discrete grid search Rather than perform a full grid search, we could‚Äôve sped up the search process by using the random discrete grid search. The following performs a random search across the same 360 hyperparameter combinations, stopping if none of the last 10 models have managed to have a 0.01% improvement in AUC compared to the best model before that. I cut the grid search off after 1200 seconds (20 minutes) if a final approximately optimal model is not found. Our grid search assessed 171 of the 360 models before stopping and the best model achieved an AUC of 0.812. # random grid search criteria search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, stopping_metric = &quot;AUC&quot;, stopping_tolerance = 0.0001, stopping_rounds = 10, max_runtime_secs = 60*20 ) # build grid search random_grid &lt;- h2o.grid( algorithm = &quot;randomForest&quot;, grid_id = &quot;rf_random_grid&quot;, x = predictors, y = response, training_frame = attrit_train_h2o, hyper_params = hyper_grid.h2o, search_criteria = search_criteria, ntrees = 500, seed = 123, nfolds = 5, keep_cross_validation_predictions = TRUE, stopping_metric = &quot;AUC&quot;, stopping_rounds = 10, stopping_tolerance = 0 ) # collect the results and sort by our model performance metric of choice random_grid_perf &lt;- h2o.getGrid( grid_id = &quot;rf_random_grid&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE ) print(random_grid_perf) ## H2O Grid Details ## ================ ## ## Grid ID: rf_random_grid ## Used hyper parameters: ## - balance_classes ## - max_depth ## - min_rows ## - mtries ## - sample_rate ## Number of models: 171 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by decreasing auc ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 1 false 30 3.0 2 0.95 rf_random_grid_model_39 0.8121580547112462 ## 2 false 10 3.0 2 0.95 rf_random_grid_model_163 0.8115421532554791 ## 3 false 20 1.0 5 0.95 rf_random_grid_model_101 0.7974137471337919 ## 4 false 30 3.0 2 0.632 rf_random_grid_model_73 0.7966458699941342 ## 5 false 15 5.0 2 0.8 rf_random_grid_model_9 0.7965312216711993 ## ## --- ## balance_classes max_depth min_rows mtries sample_rate model_ids auc ## 166 true 20 3.0 15 0.95 rf_random_grid_model_113 0.7474084146536555 ## 167 true 30 3.0 15 0.8 rf_random_grid_model_83 0.7469284914413694 ## 168 true 10 1.0 15 0.95 rf_random_grid_model_107 0.74409961072895 ## 169 true 25 1.0 15 0.95 rf_random_grid_model_63 0.7314962939263051 ## 170 true 20 1.0 15 0.95 rf_random_grid_model_142 0.7314962939263051 ## 171 true 30 1.0 15 0.95 rf_random_grid_model_61 0.7290326881032368 Once we‚Äôve identifed the best set of hyperparameters, we can extract the model. For the remaining examples I will use the optimal model from the random grid search. # Grab the model_id for the top model, chosen by validation error best_model_id &lt;- random_grid_perf@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) 4.5.2.3 Visualizing results 4.5.2.3.1 Variable importance Assessing the variable importance, we see similar results as with the ranger model with the most influential variables including OverTime, MonthlyIncome, Age, and JobRole. # plot top 25 influential variables vip(best_model, num_features = 25, bar = FALSE) 4.5.2.3.2 Partial dependence plots As with ranger, we can also assess PDPs and ICE curves. The following looks at three of the most influential variables in our model (OverTime, MonthlyIncome and Age). Our centered ICE curves help to illustrate the marginal increasing or decreasing effect on the predicted probability of attrition. In all three plots we see groups of observations going in opposite directions (i.e. at age 35 many employees experience an increase in the probability of attrition while many others a decrease). This indicates interaction effects with other features, which we will explore more in the Model Interpretability chapter. These plots illustrate similar attributes that we saw in the ranger PDP/ICE curve plots. pfun &lt;- function(object, newdata) { as.data.frame(predict(object, newdata = as.h2o(newdata)))[[3L]] } # JobRole partial dependencies ot.ice &lt;- partial( best_model, pred.var = &quot;OverTime&quot;, train = attrit_train, pred.fun = pfun ) # MonthlyIncome partial dependencies income.ice &lt;- partial( best_model, pred.var = &quot;MonthlyIncome&quot;, train = attrit_train, pred.fun = pfun, grid.resolution = 20 ) # Age partial dependencies age.ice &lt;- partial( best_model, pred.var = &quot;Age&quot;, train = attrit_train, pred.fun = pfun, grid.resolution = 20 ) p1 &lt;- autoplot(ot.ice, alpha = 0.1, center = TRUE) p2 &lt;- autoplot(income.ice, alpha = 0.1, center = TRUE) p3 &lt;- autoplot(age.ice, alpha = 0.1, center = TRUE) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) Figure 4.14: ICE curves for OverTime, MonthlyIncome, and Age. 4.5.2.3.3 ROC curve Visualizing our ROC curve helps to illustrate our cross-validated AUC of 0.812. h2o.performance(best_model, xval = TRUE) %&gt;% plot() Figure 4.15: ROC curve for our ranger random forest model based on the training data. 4.5.2.4 Predicting Finally, if you are satisfied with your final model we can predict values for an unseen data set a couple different ways. Both predict and h2o.predict will provide the predicted class and the probability of each class. We can also quickly assess the model‚Äôs performance on our test set with h2o.performance. # predict new values with base R predict() predict(best_model, attrit_test_h2o) ## predict No Yes ## 1 No 0.9205128 0.07948718 ## 2 Yes 0.6083333 0.39166666 ## 3 No 0.9163170 0.08368298 ## 4 No 0.8282505 0.17174950 ## 5 No 0.7910256 0.20897436 ## 6 No 0.9775641 0.02243590 ## ## [293 rows x 3 columns] # predict new values with h2o.predict() h2o.predict(best_model, newdata = attrit_test_h2o) ## predict No Yes ## 1 No 0.9205128 0.07948718 ## 2 Yes 0.6083333 0.39166666 ## 3 No 0.9163170 0.08368298 ## 4 No 0.8282505 0.17174950 ## 5 No 0.7910256 0.20897436 ## 6 No 0.9775641 0.02243590 ## ## [293 rows x 3 columns] # assess performance on test data h2o.performance(best_model, newdata = attrit_test_h2o) ## H2OBinomialMetrics: drf ## ## MSE: 0.1017999 ## RMSE: 0.319061 ## LogLoss: 0.3423053 ## Mean Per-Class Error: 0.2296316 ## AUC: 0.8374849 ## Gini: 0.6749697 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## No Yes Error Rate ## No 222 24 0.097561 =24/246 ## Yes 17 30 0.361702 =17/47 ## Totals 239 54 0.139932 =41/293 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.254258 0.594059 53 ## 2 max f2 0.160211 0.667752 118 ## 3 max f0point5 0.382280 0.646259 24 ## 4 max accuracy 0.382280 0.883959 24 ## 5 max precision 0.739642 1.000000 0 ## 6 max recall 0.022436 1.000000 286 ## 7 max specificity 0.739642 1.000000 0 ## 8 max absolute_mcc 0.254258 0.511808 53 ## 9 max min_per_class_accuracy 0.194833 0.744681 94 ## 10 max mean_per_class_accuracy 0.160211 0.777634 118 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.6 Implementation: Multinomial Classification 4.6.1 ranger 4.6.1.1 Basic implementation 4.6.1.2 Tuning 4.6.1.3 Visual interpretation 4.6.1.4 Predicting 4.6.2 h2o 4.6.2.1 Basic implementation 4.6.2.2 Tuning 4.6.2.3 Visual interpretation 4.6.2.4 Predicting 4.7 Learning More Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. To learn more I would start with the following resources listed in order of complexity: Practical Machine Learning with H2O An Introduction to Statistical Learning Applied Predictive Modeling Computer Age Statistical Inference The Elements of Statistical Learning References "],
["references.html", "References", " References "]
]
